Gaussian mixture models

1. Use mclust::Mclust to fit Gaussian mixture models to the zip.test
   data, for G=1 to 20 mixture components, for a fixed value of
   modelNames (use one of the diagonal models, ellipsoidal has too
   many parameters for the 256-dimensional zip.test data). Plot the
   negative log likelihood as a function of G (number of mixture
   components). The negative log likelihood should decrease as the
   number of mixture components increases. After what G does the log
   likelihood start to look flat?
   - if mclust::Mclust does not work for you (some students reported
     it returns NULL), it may be because your data set is in the wrong
     format (make sure it is a numeric matrix). Or you may try using a
     simpler covariance matrix (modelNames parameter).
   - if it is too slow on your computer, it is probably because the
     default initialization is based on hierarchical clustering
     (quadratic time in the number of observations). Instead try a
     random initializations using 
  #+BEGIN_SRC R
  Mclust(initialization = list(hcPairs=hcRandomPairs(data.mat))) 
  #+END_SRC
   - You can also try using a subset of rows (maybe only 500?) --
     results for some data are better than no result for full data!
   - You can also use if(!is.null(result)) to only store the result from Mclust if it is non-null (and ignore the result otherwise).
   - You may use set.seed(123456) or some other number, which may affect the random initialization, and whether or not the function returns a valid model.
   - You can also try using another package that implements Gaussian
     mixture models, e.g. Rmixmod, mixture::gpcm,
     EMCluster::emcluster, mixtools, bgmm, flexmix.
2. Use pdfCluster::adj.rand.index to compute the Adjusted Rand Index
   (ARI) with respect to the true class/digit labels (1 means perfect
   clustering and values near 0 mean random clusters). Make a plot of
   ARI versus number of mixture components. What is the best value of
   ARI that you observed? How many components?
3. (extra credit) Create a new plot of ARI which compares Gaussian
   mixtures to kmeans (same results as homework 2). Which of the two
   algorithms is most accurate, and at what number of clusters? Hint:
   add a column named "algorithm" (with values GMM and kmeans) to each
   of the two data tables, then combine them with rbind, and use a
   single geom_line with aes(color=algorithm) to draw each algorithm
   in a different color.

Please submit your code along with your figures and commentary in a
single PDF.

** For CS599 graduate students only:

Code the EM algorithm from scratch based on the pseudo-code in the
textbooks. 
- Write a function GMM(data.matrix, K) which returns a list similar to
  the result of the mclust::Mclust function (with at least elements
  loglik and classification).
- for the initialization, you can use a random probability matrix, or
  the cluster assignments from kmeans (which would require fewer EM
  iterations).
- for the covariance updates, make sure to use a diagonal covariance
  matrix so it is computational tractable (asymptotically linear
  rather than quadratic in the number of features), and compare with
  the equivalent modelNames parameter from mclust. See
  [[https://hal.inria.fr/inria-00074643][Gaussian parsimonious
  clustering models]], section 3.2 "The diagonal family" for
  details. Rather than using matrix multiplication %*% (which computes
  un-necessary off-diagonal entries), you can use base::colSums and
  base::diag.
- Make a figure that compares the negative log likelihood (NLL) for
  the two algorithms on the iris data (you can try zip data but it is
  quite large and there are some numerical issues you will have to
  overcome). Plot NLL as a function of K (number of clusters), using
  two random starts for each algorithm (there should be four points
  plotted per K, with different algorithms in different colors). Are
  the NLL values from your algorithm similar to those from mclust? Are
  the values similar between random starts? Does the NLL increase as a
  function of the number of clusters, as expected?
- To reduce repetition in your code please use three nested
  for loops: (1) K=1 to 20, (2) algorithm=yours or mclust, (3) random
  seed=1 or 2, e.g.

#+BEGIN_SRC R
  algo.list <- list(mine=GMM, stats=mclust::Mclust)
  ss.dt.list <- list()
  for(K in 1:20){
    for(algo.name in names(algo.list)){
      for(seed in 1:2){
	set.seed(seed)
	algo.fun <- algo.list[[algo.name]]
	result.list <- algo.fun(X, K)
	ss.dt.list[[paste(K, algo.name, seed)]] <- data.table(
	  K, algo.name, seed, NLL=-result.list$loglik)
      }
    }
  }
#+END_SRC

Extra credit: code an accelerated version of EM, as described in the
[[https://cloud.r-project.org/web/packages/turboEM/vignettes/turboEM.pdf][turboEM package vignette]].

