The goal of this homework is to use an auto-encoder to learn a low
dimensional nonlinear mapping of a high dimensional data set, and
compare to the PCA linear mapping.

1. first select 100 rows of the zip.train data from the ESL book (10
   from each class). For these data use the keras R package to define
   an auto-encoder, using keras::keras_model_sequential and
   keras::layer_dense(activation="relu" or "sigmoid") for the
   intermediate layers. For visualization purposes make sure the code
   layer has two units. How many parameters are there to learn in this
   nonlinear model? How many parameters are there to learn in the
   corresponding PCA linear model with rank=2?  Is the number of
   parameters in the auto-encoder larger as expected?

2. Now learn the auto-encoder parameters using keras::compile (with
   keras::loss_mean_squared_error) and keras::fit. Use the predict
   function to compute the predicted values. Also compute a PCA with
   rank=2 and compute its predicted values. What is the reconstruction
   error (mean squared error between data and predicted values) for
   the two methods? Is the auto-encoder more accurate as expected? If
   not, try increasing the number epochs, and/or the learning rate (lr
   parameter of optimizer_sgd).

3. Now use keras::keras_model, keras::get_layer, and predict functions
   to compute the low-dimensional embedding of the original train
   data. Make a ggplot with these auto-encoder embeddings in one
   panel, and the PCA in another panel, using
   facet_wrap(scales="free") so that the plots are NOT constrained to
   have the same scales on the x/y axes (the units of the PCA and
   auto-encoder embeddings are not comparable). Use geom_text(label=digit) or
   geom_point(color=digit) to visualize the different digit
   classes. Which of the two methods results in better separation
   between digit classes?

** CS599 graduate students only

Your job is to investigate how the auto-encoder model architecture
affects overfitting.
- First decide on two different auto-encoder architectures of varying
  complexity that you would like to compare. For example you may
  compare a (256,100,10,2,10,100,256) to (256,10,2,10,256) to see if
  adding layers affects overfitting. Or you could compare
  (256,10,2,10,256) to (256,100,2,100,256) to see if the number of
  intermediate units affects overfitting.
- Create a variable named model.list, which should be a list of the
  two keras models described above. Make a for loop over these two
  models, and use keras::fit(validation_split=0.5) to learn parameters
  for each model using a 50% subtrain, 50% validation split.
- Make a ggplot of y=square loss as a function of x=iterations, with
  different sets in different colors (e.g., subtrain=black,
  validation=red), and the two different models in two different
  panels, facet_grid(. ~ model). Does either model overfit?
- Finally make another ggplot which displays the low dimensional
  embeddings, as in problem 3 above. Which of the two methods results
  in better separation between digit classes?
