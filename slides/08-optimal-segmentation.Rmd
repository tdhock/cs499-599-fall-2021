---
title: "Optimal segmentation"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
setkey(nb.dt, profile.id, chromosome)
one.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position, logratio),
    data=one.dt)
```

---

# Motivation for optimal detection

- Binary segmentation is a greedy algorithm, so sometimes it chooses a
  changepoint which is sub-optimal for a larger model size.
- Is it possible to compute the changepoints and segments which are
  optimal for each model size? Yes, with dynamic programming. 
- Dynamic programming can be applied to any computational problem
  which involves optimization, and can be broken down into independent
  sub-problems. In this context we use it to compute the changepoints
  and segments which are optimal for a given loss function.
- Is it desirable to compute the optimal changepoints and segments?
  Yes, see examples in next slides.
  
---

# Example 1: stuck with sub-optimal change
  
```{r}
get_segs_loss <- function(pid, chrom, max.segments=6){
  one <- nb.dt[J(pid, chrom)]
  logratio <- one[["logratio"]]
  binseg <- binsegRcpp::binseg_normal(logratio, max.segments)
  opt <- jointseg::Fpsn(logratio, max.segments)
  cum.end <- cumsum(logratio)
  cum.start <- c(0,cum.end)
  end <- t(opt[["t.est"]])
  start <- rbind(1, end[-max.segments,]+1)
  n.data <- end-start+1
  seg.sum <- cum.end[end]-cum.start[start]
  ldt <- function(algorithm, loss){
    data.table(algorithm=a(algorithm), segments=seq_along(loss), loss)
  }
  a <- function(x)factor(x, c("optimal", "binseg"))
  loss.dt <- rbind(
    ldt("optimal", opt$J.est),
    ldt("binseg", binseg$loss))
  segs.dt <- rbind(
    data.table(
      algorithm=a("optimal"),
      segments=as.integer(col(end)),
      start=as.integer(start),
      end=as.integer(end),
      mean=as.numeric(seg.sum/n.data)
    )[!is.na(mean)],
    data.table(algorithm=a("binseg"), coef(binseg)))
  one[, data.i := .I]
  change.dt <- segs.dt[1 < start]
  list(segment=segs.dt, change=change.dt, loss=loss.dt, data=one)
}

models <- get_segs_loss("565", "X")
algo.sizes <- c(optimal=1.25, binseg=0.5)
gg <- ggplot()+
  geom_text(aes(
    1000, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1,
    data=models[["loss"]])+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to start

```{r}
gg+coord_cartesian(xlim=c(0, 200))
```

---

# Zoom to center

```{r}
gg+coord_cartesian(xlim=c(2100, 2300))
```

---

# Zoom to end

```{r}
gg+coord_cartesian(xlim=c(3700, 3800))
```

---

# Example 2: missing a small change down

```{r}
models <- get_segs_loss("229", "X")
gg <- ggplot()+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  geom_text(aes(
    1000, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1,
    data=models[["loss"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to start

```{r}
gg+coord_cartesian(xlim=c(0, 200))
```

---

# Zoom to center

```{r}
gg+coord_cartesian(xlim=c(2000, 2500))
```

---

# Zoom to end

```{r}
gg+coord_cartesian(xlim=c(3700, 3800))
```

---

# Example 3: difficult to detect an outlier

```{r}
models <- get_segs_loss("590", "1")
gg <- ggplot()+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  geom_label(aes(
    2000, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1,
    data=models[["loss"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom X axis to outlier

```{r}
gg+coord_cartesian(xlim=c(5000, 5100))
```

---

# Example 4: stuck with sub-optimal change 

```{r}
models <- get_segs_loss("2", "2", 4)
gg <- ggplot()+
  geom_text(aes(
    200, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1,
    data=models[["loss"]])+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to changepoint

```{r}
gg+coord_cartesian(xlim=c(50, 100))
```

---

# Dynamic programming for optimal changepoint detection

- We have $n$ data $z_1, \dots, z_n$.
- Fix the number of segments $S\in\{1, 2, \dots, n\}$.
- Optimization variables: $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment means
    $u_1,\dots,u_S\in\mathbb R$ ($t_0=0$, $t_S=n$).
- Statistical model: for every segment $s\in\{1,\dots,S\}$,
    $z_i \stackrel{\text{iid}}{\sim} N(u_s, \sigma^2)$ for every
    data point $i\in(t_{s-1},t_s]$ implies square loss function
    $\ell(u_s, z_i)=(u_s-z_i)^2$ to minimize.

```{r plotcos, dev='tikz', fig.height=4}
max.segments <- 4
logratio <- one.dt[["logratio"]]
opt <- jointseg::Fpsn(logratio, max.segments)
cum.end <- cumsum(logratio)
cum.start <- c(0,cum.end)
end <- t(opt[["t.est"]])
start <- rbind(1, end[-max.segments,]+1)
n.data <- end-start+1
seg.sum <- cum.end[end]-cum.start[start]
segs.dt <- data.table(
  segments=as.integer(col(end)),
  start=as.integer(start),
  end=as.integer(end),
  mean=as.numeric(seg.sum/n.data)
)[!is.na(mean)]
one.dt[, data.i := .I]

show.segs <- segs.dt[segments==max.segments]
show.changes <- show.segs[1 < start]
show.changes[, change.i := .I]
show.segs[, seg.i := .I]
model.color <- "blue"
text.size <- 5
theme.big <- theme(text=element_text(size=20))
ggplot()+
  theme.big+
  ggtitle(sprintf(
    "Optimal model for $n=%d$ data and $S=%d$ segments",
    nrow(one.dt), max.segments))+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=one.dt)+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    color=model.color,
    size=1,
    data=show.segs)+
  geom_vline(aes(
    xintercept=start-0.5),
    data=show.changes,
    size=1,
    color=model.color)+
  coord_cartesian(xlim=c(-20, max(one.dt[["data.i"]])))+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio = noisy DNA\ncopy number measurement")+
  geom_text(aes(
    start+0.5, -Inf,
    label=sprintf("$t_%d=%d$", change.i, start-1)),
    color=model.color,
    size=text.size,
    hjust=0,
    vjust=-0.5,
    data=show.changes)+
  geom_text(aes(
    start-1.5, mean,
    label=sprintf("$u_%d=%.4f$", seg.i, mean)),
    color=model.color,
    size=text.size,
    hjust=1,
    data=show.segs)

```

---

# Maximum likelihood inference for $S$ segments and $n$ data

The best loss for $S$ segments and $n$ data is
```{=latex}
\begin{eqnarray*}
  \mathcal L_{S,n} &=& \min_{\substack{
  \mathbf u\in\mathbb R^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
\\
&=&\min_{t_{S-1}}\underbrace{
\min_{\substack{
u_1,\dots,u_{S-1}\\
t_1<\cdots<t_{S-2}
}}
  \sum_{s=1}^{S-1}\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i)
}_{
\mathcal L_{S-1, t_{S-1}}
} +
\underbrace{\min_{u_S} \sum_{i=t_{S-1}+1}^{t_S=n} \ell( u_S,  z_i)}_{
c_{(t_{S-1}, t_S=n]}
}
%      \text{subject to \hskip 0.75cm} &\ \ \alert<3>{u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\},}  \nonumber\\
%  &\ \ \alert<3>{u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\}.}  \nonumber 
\end{eqnarray*}
```

- Hard optimization problem because of integer-valued changepoint $t_s$
  variables, naively $O(n^S)$ time.
- Auger and Lawrence (1989): $O(Sn^2)$ time classical dynamic
  programming algorithm (best loss computed recursively).
$$
\mathcal L_{s,t}=\min_{t'<t} \mathcal L_{s-1,t'} + c_{(t', t]}
$$

```{r, dev='tikz', results="asis"}
N.data <- length(logratio)
loss.mat <- matrix(NA, N.data, max.segments)
cum.mat <- rbind(
  data=cumsum(logratio),
  square=cumsum(logratio^2),
  N=1:N.data)
getLoss <- function(mat){  
  data.table(t(mat))[, `:=`(
    mean=data/N,
    loss=square-data^2/N
  )][]
}
loss.dt <- getLoss(cum.mat)
loss.mat[,1] <- loss.dt[["loss"]]
gg <- ggplot()+
  theme.big+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "")+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=data.table(panel="data", one.dt))+
  geom_point(aes(
    N+0.5, loss),
    shape=1,
    color=model.color,
    data=data.table(panel="loss", loss.dt))+
  geom_blank(aes(x,y), data=data.table(panel="loss", x=270, y=-3))+
  facet_grid(panel ~ ., scales="free")
loss1 <- function(end){
  seg.dt <- loss.dt[end]
  
  cat("
$$
\\mathcal L_{1,t}= c_{(0, t]}
$$
")
  some.tiles <- data.table(loss.tiles)
  some.tiles[data.i > end | segments > 1, loss := NA]
  gg+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      size=1,
      data=seg.dt)+
    geom_raster(aes(
      data.i+0.5, segments,
      fill=loss),
      data=data.table(some.tiles, panel="matrix"))+
    geom_tile(aes(
      data.i+0.5, segments),
      color="black",
      fill="transparent",
      data=data.table(loss.tiles, panel="matrix"))+
    scale_fill_gradient(
      low="white", high="red", 
      limits=range(loss.tiles$loss, na.rm=TRUE))+
    geom_text(aes(
      end+0.5,
      loss,
      ##10,
      label=sprintf("$\\mathcal L_{1,%d}=%.2f$", end, loss)),
      color=model.color,
      ##hjust=if(end<(nrow(one.dt)/2))0 else 1,
      hjust=0,
      vjust=1.3,
      size=text.size,
      data=data.table(panel="loss", seg.dt))+
    geom_segment(aes(
      0.5, mean,
      xend=end+0.5, yend=mean),
      data=data.table(panel="data", seg.dt),
      size=1,
      color=model.color)
}
best.segs.list <- split(
  loss.dt[, .(start=1, end=N, mean)],
  paste(1, loss.dt[["N"]]))
loss.list <- list()
for(n.segs in 2:max.segments){
  for(up.to in n.segs:N.data){
    last.start <- n.segs:up.to
    prev.end <- last.start-1
    last.dt <- getLoss(cum.mat[,up.to] - cum.mat[,prev.end])
    loss.choices <- data.table(
      prev.end,
      last_mean=last.dt[["mean"]],
      prev_loss=loss.mat[prev.end,n.segs-1],
      last_loss=last.dt[["loss"]])
    loss.choices[, total_loss := prev_loss + last_loss ]
    loss.list[[paste(n.segs, up.to)]] <- loss.choices
    best.choice <- loss.choices[which.min(total_loss)]
    prev.segs <- best.segs.list[[
      paste(n.segs-1, best.choice[["prev.end"]])
      ]]
    last.seg <- best.choice[, .(
      start=prev.end+1, end=up.to, mean=last_mean)]
    best.segs.list[[paste(n.segs, up.to)]] <- rbind(
      prev.segs, last.seg)
    loss.mat[up.to,n.segs] <- best.choice[["total_loss"]]
  }
}
loss.tiles <- data.table(
  data.i=as.integer(row(loss.mat)),
  segments=as.integer(col(loss.mat)),
  loss=as.numeric(loss.mat))
loss.tiles[loss<0, loss := 0]

showChoice <- function(up.to, n.segs, choice){
  some.tiles <- data.table(loss.tiles)
  some.tiles[(segments==n.segs & data.i>up.to) | segments>n.segs, loss := NA]
  cat(sprintf("
$$
\\mathcal L_{%d,t}= \\min_{t'<t} \\mathcal L_{%d,t'} + c_{(t', t]}
$$
", n.segs, n.segs-1))
  loss.choices <- loss.list[[paste(n.segs, up.to)]]
  chosen.loss <- loss.choices[prev.end==choice]
  prev.segs <- best.segs.list[[paste(n.segs-1, choice)]]
  show.segs <- rbind(
    data.table(type="prev", prev.segs),
    chosen.loss[, .(
      type="last",
      start=prev.end+1,
      end=up.to,
      mean=last_mean)])
  loss.tall <- nc::capture_melt_single(
    loss.choices,
    type=".*?",
    "_loss")
  chosen.loss.tall <- loss.tall[prev.end==choice]
  loss.min <- loss.tall[type=="total"][which.min(value)]
  ## type.colors <- c(
  ##   last=model.color,
  ##   prev="red",
  ##   total="black")
  gg <- ggplot()+
    theme.big+
    scale_x_continuous(
      "Position/index in data sequence")+
    scale_y_continuous(
      "")+
    geom_point(aes(
      data.i, logratio),
      shape=1,
      data=data.table(panel="data", one.dt))+
    geom_point(aes(
      prev.end+0.5, value, color=type),
      shape=1,
      data=data.table(panel="loss", loss.tall))+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean,
      color=type),
      size=2,
      data=data.table(panel="data", show.segs))+
    geom_vline(aes(
      xintercept=prev.end+0.5, 
      color=type),
      data=data.table(chosen.loss, type="total"))+
    geom_vline(aes(
      xintercept=data.i),
      data=data.table(data.i=up.to))+
    geom_raster(aes(
      data.i, segments,
      fill=loss),
      data=data.table(some.tiles, panel="matrix"))+
    geom_tile(aes(
      data.i, segments),
      color="black",
      fill="transparent",
      data=data.table(loss.tiles, panel="matrix"))+
    scale_fill_gradient(
      low="white", high="red",
      limits=range(loss.tiles$loss, na.rm=TRUE))+
    geom_text(aes(
      data.i, Inf,
      hjust=ifelse(data.i<N.data/2, 0, 1),
      label=sprintf("$t=%d$", data.i)),
      size=text.size,
      vjust=1,
      data=data.table(data.i=up.to, panel="data"))+
    geom_text(aes(
      prev.end+0.5, -Inf,
      color=type,
      label=sprintf("$t'=%d$", prev.end)),
      size=text.size,
      hjust=0,
      vjust=0,
      data=data.table(chosen.loss, type="total", panel="data"))+
    ## geom_text(aes(
    ##   prev.end+0.5,
    ##   value,
    ##   color=type,
    ##   label=sprintf("$\\mathcal L_{%d,%d}=%.2f$", n.segs-1, prev.end, value)),
    ##   hjust=0,
    ##   vjust=1.3,
    ##   size=text.size,
    ##   data=data.table(panel="loss", chosen.loss.tall))+
    ## geom_text(aes(
    ##   -Inf,
    ##   value,
    ##   color=type,
    ##   label=sprintf("$\\mathcal L_{%d,%d}=%.2f$", n.segs, up.to, value)),
    ##   hjust=0,
    ##   vjust=1,
    ##   size=text.size,
    ##   data=data.table(panel="loss", loss.min))+
    geom_label(aes(
      prev.end+0.5,
      value,
      color=type,
      label=sprintf(
        "$t^*=%d$",
        prev.end)),
      ##vjust=1, hjust=0.5,
      vjust=0.5, hjust=1,
      label.size=0,
      alpha=0.5,
      size=text.size,
      data=data.table(panel="loss", loss.min))+
    geom_label(aes(
      prev.end+0.5,
      value,
      color=type,
      label=sprintf(
        "$\\mathcal L_{%d,%d}=%.2f$",
        n.segs, up.to, value)),
      ##vjust=2, hjust=0.5,
      vjust=0.5, hjust=0,
      label.size=0,
      alpha=0.5,
      size=text.size,
      data=data.table(panel="loss", loss.min))+
    geom_point(aes(
      prev.end+0.5,
      value,
      color=type),
      data=data.table(panel="loss", loss.min))+
    ## geom_hline(aes(
    ##   yintercept=value,
    ##   color=type),
    ##   data=data.table(panel="loss", loss.min))+
    ##geom_blank(aes(x,y), data=data.table(panel="loss", x=0, y=-1))+
    facet_grid(panel ~ ., scales="free")
  gg
}
```

---

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(1)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(10)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(100)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(nrow(one.dt))
```

---

# Efficient computation of c values

The best square loss up to $t$ is
$$
c_{(0, t]} = \min_u \sum_{i=1}^t (u - z_i)^2 =
t\hat u_t^2 - 2\hat u_t\sum_{i=1}^t z_i + \sum_{i=1}^t z_i^2.
$$

We can use cumulative sum to compute $S_t$ for all $t$ in linear
$O(t)$ time,
$S_t = \sum_{i=1}^t z_i.$

As can all best means up to $t$,
$\hat u_t = S_t / t.$

As can cumulative sum of squares,
$Q_t = \sum_{i=1}^t z_i^2.$

All best loss values up to $t$ can also be computed in linear time,
$$ 
t\hat u_t^2 - 2\hat u_t\sum_{i=1}^t z_i + \sum_{i=1}^t z_i^2 =
t (S_t/t)^2 - 2(S_t/t)(S_t) + Q_t = 
Q_t-S_t^2/t.
$$

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(30, 2, 10)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(30, 2, 16)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(30, 2, 25)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(100, 2, 10)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(100, 2, 41)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(100, 2, 80)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 10)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 41)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 80)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 150)
```

---

# Computational complexity analysis

```{=latex}
\begin{tabular}{cccc}
segments & grid search & dynamic programming & binary segmentation \\
\hline
1 & $O(n)$ & $O(n)$ & $O(n)$ \\
2 & $O(n^2)$ & $O(n^2)$ & $O(\log n)$--$O(n)$\\
3 & $O(n^3)$ & $O(n^2)$ & $O(\log n)$--$O(n)$\\
4 & $O(n^4)$ & $O(n^2)$ & $O(\log n)$--$O(n)$\\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ 
\end{tabular}
```

For example with $n = `r N.data`$ we have $\log n = `r log(N.data)`$,
$n^2 = `r N.data^2`$ and $n^3 = `r N.data^3`$.

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(150, 3, 20)
```

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(150, 3, 113)
```

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(150, 3, 130)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 50)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 100)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 157)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 200)
```

---

# Comparison with previous approximate/heuristic algorithms

Every other algorithm we have seen so far has been
approximate/heuristic, because it is not guaranteed to compute a
global optimum of the objective function.

- K-means: local minimum squared error.
- EM algo for Gaussian mixtures: local maximum likelihood.
- Binary segmentation: local minimum squared error
  (maximum Gaussian likelihood).

In contrast the dynamic programming algorithm is guaranteed to compute
a solution (segment means and changepoints) which is globally optimal
(maximum Gaussian likelihoood, minimum squared error).

Dynamic programming is $O(n^2 S)$ for $S$ segments and $n$ data,
whereas binary segmentation is $O(n \log S)$ best case and $O(n S)$
worst case. (optimal algorithms are typically slower than
approximate/heuristic algorithms)

---

# Possible exam questions

- For $S$ segments how does the binary segmentation loss compare to
  the dynamic programming loss? Answer for each $S\in\{1,2,3,4\}$ one
  or more of: less than, equal, and/or greater than.
- How many values for the last changepoint variable are considered in
  the computation of $\mathcal L_{3,100}$ ?
- Is {binary segmentation, dynamic programming} an {optimal,
  approximate} algorithm? Why?
