---
title: "Optimal segmentation"
author: "Toby Dylan Hocking"

output:
    beamer_presentation:
        includes:
            in_header: 08-optimal-segmentation-colors.tex
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

```{r colors}
model.color <- "blue"
type.colors <- c(
  last="deepskyblue",
  prev="red",
  total=model.color)
type.colors.mat <- col2rgb(type.colors)/255
type.colors.list <- apply(type.colors.mat, 1, identity, simplify=FALSE)
type.colors.hex <- do.call(rgb, type.colors.list)
type.colors.latex <- sprintf(
  "\\definecolor{%s}{HTML}{%s}",
  names(type.colors),
  sub("#", "", type.colors.hex))
##\definecolor{Mycolor2}{HTML}{00F9DE} https://www.overleaf.com/learn/latex/Using_colours_in_LaTeX
cat(type.colors.latex, file="08-optimal-segmentation-colors.tex", sep="\n")
## https://tex.stackexchange.com/questions/171711/how-to-include-latex-package-in-r-markdown
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
setkey(nb.dt, profile.id, chromosome)
one.dt <- nb.dt[J("4", "2")]
one.dt[, data.i := .I]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    data.i, logratio),
    data=one.dt)
```

---

# Motivation for optimal detection

- Binary segmentation is a greedy algorithm, so sometimes it chooses a
  changepoint which is sub-optimal for a larger model size.
- Is it possible to compute the changepoints and segments which are
  optimal for each model size? Yes, with dynamic programming. 
- Dynamic programming can be applied to any computational problem
  which involves optimization, and can be broken down into independent
  sub-problems. In this context we use it to compute the changepoints
  and segments which are optimal for a given loss function.
- Is it desirable to compute the optimal changepoints and segments?
  Yes, see examples in next slides.
  
---

# Example 1: stuck with sub-optimal change
  
```{r fig.height=8}
get_segs_loss <- function(pid, chrom, max.segments=6){
  one <- nb.dt[J(pid, chrom)]
  logratio <- one[["logratio"]]
  binseg <- binsegRcpp::binseg_normal(logratio, max.segments)
  opt <- jointseg::Fpsn(logratio, max.segments)
  cum.end <- cumsum(logratio)
  cum.start <- c(0,cum.end)
  end <- t(opt[["t.est"]])
  start <- rbind(1, end[-max.segments,]+1)
  n.data <- end-start+1
  seg.sum <- cum.end[end]-cum.start[start]
  ldt <- function(algorithm, loss){
    data.table(algorithm=a(algorithm), segments=seq_along(loss), loss)
  }
  a <- function(x)factor(x, c("optimal", "binseg"))
  loss.dt <- rbind(
    ldt("optimal", opt$J.est),
    ldt("binseg", binseg$loss))
  segs.dt <- rbind(
    data.table(
      algorithm=a("optimal"),
      segments=as.integer(col(end)),
      start=as.integer(start),
      end=as.integer(end),
      mean=as.numeric(seg.sum/n.data)
    )[!is.na(mean)],
    data.table(algorithm=a("binseg"), coef(binseg)))
  one[, data.i := .I]
  change.dt <- segs.dt[1 < start]
  list(segment=segs.dt, change=change.dt, loss=loss.dt, data=one)
}

models <- get_segs_loss("565", "X")
algo.sizes <- c(optimal=1.25, binseg=0.5)
theme.mid <- theme(text=element_text(size=18))
gg <- ggplot()+
  theme.mid+
  geom_text(aes(
    1200, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1.1,
    data=models[["loss"]])+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to start

```{r fig.height=8}
gg+coord_cartesian(xlim=c(0, 300))
```

---

# Zoom to center

```{r fig.height=8}
gg+coord_cartesian(xlim=c(2100, 2300))
```

---

# Zoom to end

```{r fig.height=8}
gg+coord_cartesian(xlim=c(3700, 3800))
```

---

# Example 2: missing a small change down

```{r fig.height=8}
models <- get_segs_loss("229", "X")
gg <- ggplot()+
  theme.mid+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  geom_text(aes(
    1200, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1.1,
    data=models[["loss"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to start

```{r fig.height=8}
gg+coord_cartesian(xlim=c(0, 200))
```

---

# Zoom to center

```{r fig.height=8}
gg+coord_cartesian(xlim=c(2000, 2500))
```

---

# Zoom to end

```{r fig.height=8}
gg+coord_cartesian(xlim=c(3700, 3800))
```

---

# Example 3: difficult to detect an outlier

```{r fig.height=8}
models <- get_segs_loss("590", "1")
gg <- ggplot()+
  theme.mid+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  geom_label(aes(
    2000, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1.1,
    data=models[["loss"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom X axis to outlier

```{r fig.height=8}
gg+coord_cartesian(xlim=c(5000, 5100))
```

---

# Example 4: stuck with sub-optimal change 

```{r fig.height=8}
models <- get_segs_loss("2", "2", 4)
gg <- ggplot()+
  theme.mid+
  geom_text(aes(
    200, Inf,
    color=algorithm,
    hjust=ifelse(algorithm=="binseg", 0, 1),
    label=sprintf("%s loss=%.2f", algorithm, loss)),
    vjust=1.1,
    data=models[["loss"]])+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=models[["data"]])+
  geom_segment(aes(
    start-0.5, mean,
    color=algorithm,
    size=algorithm,
    xend=end+0.5, yend=mean),
    data=models[["segment"]])+
  geom_vline(aes(
    xintercept=start-0.5,
    size=algorithm,
    color=algorithm),
    data=models[["change"]])+
  scale_size_manual(values=algo.sizes)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  facet_grid(segments ~ ., labeller=label_both)
gg
```

---

# Zoom to changepoint

```{r fig.height=8}
gg+coord_cartesian(xlim=c(50, 100))
```

---

# Dynamic programming for optimal changepoint detection

- We have $n$ data $z_1, \dots, z_n$.
- Fix the number of segments $S\in\{1, 2, \dots, n\}$.
- Optimization variables: $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment means
    $u_1,\dots,u_S\in\mathbb R$ ($t_0=0$, $t_S=n$).
- Statistical model: for every segment $s\in\{1,\dots,S\}$,
    $z_i \stackrel{\text{iid}}{\sim} N(u_s, \sigma^2)$ for every
    data point $i\in(t_{s-1},t_s]$ implies square loss function
    $\ell(u_s, z_i)=(u_s-z_i)^2$ to minimize.

```{r dev='tikz', fig.height=4}
max.segments <- 4
logratio <- one.dt[["logratio"]]
opt <- jointseg::Fpsn(logratio, max.segments)
cum.end <- cumsum(logratio)
cum.start <- c(0,cum.end)
end <- t(opt[["t.est"]])
start <- rbind(1, end[-max.segments,]+1)
n.data <- end-start+1
seg.sum <- cum.end[end]-cum.start[start]
segs.dt <- data.table(
  segments=as.integer(col(end)),
  start=as.integer(start),
  end=as.integer(end),
  mean=as.numeric(seg.sum/n.data)
)[!is.na(mean)]
one.dt[, data.i := .I]

show.segs <- segs.dt[segments==max.segments]
show.changes <- show.segs[1 < start]
show.changes[, change.i := .I]
show.segs[, seg.i := .I]
text.size <- 5
theme.big <- theme(text=element_text(size=20))
ggplot()+
  theme.big+
  ggtitle(sprintf(
    "Optimal model for $n=%d$ data and $S=%d$ segments",
    nrow(one.dt), max.segments))+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=one.dt)+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    color=model.color,
    size=1,
    data=show.segs)+
  geom_vline(aes(
    xintercept=start-0.5),
    data=show.changes,
    size=1,
    color=model.color)+
  coord_cartesian(xlim=c(-20, max(one.dt[["data.i"]])))+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio = noisy DNA\ncopy number measurement")+
  geom_text(aes(
    start+0.5, -Inf,
    label=sprintf("$t_%d=%d$", change.i, start-1)),
    color=model.color,
    size=text.size,
    hjust=0,
    vjust=-0.5,
    data=show.changes)+
  geom_text(aes(
    start-1.5, mean,
    label=sprintf("$u_%d=%.4f$", seg.i, mean)),
    color=model.color,
    size=text.size,
    hjust=1,
    data=show.segs)

```

---

# Maximum likelihood inference for $S$ segments and $n$ data

The best loss for $S$ segments and $n$ data is
```{=latex}
\begin{eqnarray*}
  \mathcal L_{S,n} &=& \min_{\substack{
  \mathbf u\in\mathbb R^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
\\
&=&\min_{t_{S-1}}\underbrace{
\min_{\substack{
u_1,\dots,u_{S-1}\\
t_1<\cdots<t_{S-2}
}}
  \sum_{s=1}^{S-1}\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i)
}_{
\mathcal L_{S-1, t_{S-1}}
} +
\underbrace{\min_{u_S} \sum_{i=t_{S-1}+1}^{t_S=n} \ell( u_S,  z_i)}_{
c_{(t_{S-1}, t_S=n]}
}
%      \text{subject to \hskip 0.75cm} &\ \ \alert<3>{u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\},}  \nonumber\\
%  &\ \ \alert<3>{u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\}.}  \nonumber 
\end{eqnarray*}
```

- Hard optimization problem because of integer-valued changepoint $t_s$
  variables, naively $O(n^S)$ time.
- Auger and Lawrence (1989): $O(Sn^2)$ time classical dynamic
  programming algorithm (best loss computed recursively).
$$
\mathcal L_{s,t}=\min_{t'<t} \mathcal L_{s-1,t'} + c_{(t', t]}
$$

```{r, dev='tikz', results="asis"}
N.data <- length(logratio)
loss.mat <- matrix(NA, N.data, max.segments)
cum.mat <- rbind(
  data=cumsum(logratio),
  square=cumsum(logratio^2),
  N=1:N.data)
getLoss <- function(mat){  
  data.table(t(mat))[, `:=`(
    mean=data/N,
    loss=square-data^2/N
  )][]
}
loss.dt <- getLoss(cum.mat)
loss.mat[,1] <- loss.dt[["loss"]]
pfac <- function(x)factor(x, c("data values", "candidate loss", "segments"))
gg <- ggplot()+
  theme.big+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "")+
  geom_point(aes(
    data.i, logratio),
    shape=1,
    data=data.table(panel=pfac("data values"), one.dt))+
  geom_point(aes(
    N+0.5, loss),
    shape=1,
    color=model.color,
    data=data.table(panel=pfac("candidate loss"), loss.dt))+
  geom_blank(aes(
    x,y),
    data=data.table(panel=pfac("candidate loss"), x=270, y=-3))+
  facet_grid(panel ~ ., scales="free")

best.segs.list <- split(
  loss.dt[, .(start=1, end=N, mean)],
  paste(1, loss.dt[["N"]]))
loss.list <- list()
for(n.segs in 2:max.segments){
  for(up.to in n.segs:N.data){
    last.start <- n.segs:up.to
    prev.end <- last.start-1
    last.dt <- getLoss(cum.mat[,up.to] - cum.mat[,prev.end])
    loss.choices <- data.table(
      prev.end,
      last_mean=last.dt[["mean"]],
      prev_loss=loss.mat[prev.end,n.segs-1],
      last_loss=last.dt[["loss"]])
    loss.choices[, total_loss := prev_loss + last_loss ]
    loss.list[[paste(n.segs, up.to)]] <- loss.choices
    best.choice <- loss.choices[which.min(total_loss)]
    prev.segs <- best.segs.list[[
      paste(n.segs-1, best.choice[["prev.end"]])
      ]]
    last.seg <- best.choice[, .(
      start=prev.end+1, end=up.to, mean=last_mean)]
    best.segs.list[[paste(n.segs, up.to)]] <- rbind(
      prev.segs, last.seg)
    loss.mat[up.to,n.segs] <- best.choice[["total_loss"]]
  }
}
loss.tiles <- data.table(
  data.i=as.integer(row(loss.mat)),
  segments=as.integer(col(loss.mat)),
  loss=as.numeric(loss.mat))
loss.tiles[loss<0, loss := 0]
scale.fill.loss <- scale_fill_gradient(
  "optimal\nloss",
  low="white", high=model.color,
  guide=guide_colorbar(order=2),
  limits=range(loss.tiles$loss, na.rm=TRUE))
loss1 <- function(end){
  seg.dt <- loss.dt[end]
  cat("
$$
{ \\color{total} \\mathcal L_{1,t}= c_{(0, t]} }
$$
")
  some.tiles <- data.table(loss.tiles)
  some.tiles[data.i > end | segments > 1, loss := NA]
  gg+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      size=1,
      data=seg.dt)+
    geom_raster(aes(
      data.i+0.5, segments,
      fill=loss),
      data=data.table(some.tiles, panel=pfac("segments")))+
    geom_tile(aes(
      data.i+0.5, segments),
      color="black",
      fill="transparent",
      data=data.table(loss.tiles, panel=pfac("segments")))+
    scale.fill.loss+
    geom_text(aes(
      end+0.5,
      loss,
      ##10,
      label=sprintf("$\\mathcal L_{1,%d}=%.2f$", end, loss)),
      color=model.color,
      ##hjust=if(end<(nrow(one.dt)/2))0 else 1,
      hjust=0,
      vjust=1.3,
      size=text.size,
      data=data.table(panel=pfac("candidate loss"), seg.dt))+
    geom_segment(aes(
      0.5, mean,
      xend=end+0.5, yend=mean),
      data=data.table(panel=pfac("data values"), seg.dt),
      size=1,
      color=model.color)
}
showChoice <- function(up.to, n.segs, choice){
  some.tiles <- data.table(loss.tiles)
  some.tiles[(segments==n.segs & data.i>up.to) | segments>n.segs, loss := NA]
  cat(sprintf("
$$
{ \\color{total} \\mathcal L_{%d,t} }= \\min_{t'<t}
{ \\color{prev} \\mathcal L_{%d,t'} }
 +
{ \\color{last} c_{(t', t]} }
$$
", n.segs, n.segs-1))
  loss.choices <- loss.list[[paste(n.segs, up.to)]]
  chosen.loss <- loss.choices[prev.end==choice]
  prev.segs <- best.segs.list[[paste(n.segs-1, choice)]]
  show.segs <- rbind(
    data.table(type="prev", prev.segs),
    chosen.loss[, .(
      type="last",
      start=prev.end+1,
      end=up.to,
      mean=last_mean)])
  loss.tall <- nc::capture_melt_single(
    loss.choices,
    type=".*?",
    "_loss")
  chosen.loss.tall <- loss.tall[prev.end==choice]
  loss.min <- loss.tall[type=="total"][which.min(value)]
  gg <- ggplot()+
    theme.big+
    scale_color_manual(
      "loss type",
      values=type.colors,
      guide=guide_legend(order=1),
      )+
    scale.fill.loss+
    scale_x_continuous(
      "Position/index in data sequence")+
    scale_y_continuous(
      "")+
    geom_point(aes(
      data.i, logratio),
      shape=1,
      data=data.table(panel=pfac("data values"), one.dt))+
    geom_point(aes(
      prev.end+0.5, value, color=type),
      shape=1,
      data=data.table(panel=pfac("candidate loss"), loss.tall))+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean,
      color=type),
      size=2,
      data=data.table(panel=pfac("data values"), show.segs))+
    geom_vline(aes(
      xintercept=prev.end+0.5, 
      color=type),
      data=data.table(chosen.loss, type="total"))+
    geom_vline(aes(
      xintercept=data.i),
      data=data.table(data.i=up.to))+
    geom_raster(aes(
      data.i, segments,
      fill=loss),
      data=data.table(some.tiles, panel=pfac("segments")))+
    geom_tile(aes(
      data.i, segments),
      color="black",
      fill="transparent",
      data=data.table(loss.tiles, panel=pfac("segments")))+
    geom_text(aes(
      data.i, Inf,
      hjust=ifelse(data.i<N.data/2, 0, 1),
      label=sprintf("$t=%d$", data.i)),
      size=text.size,
      vjust=1.1,
      data=data.table(data.i=up.to, panel=pfac("data values")))+
    geom_text(aes(
      prev.end+0.5, -Inf,
      color=type,
      label=sprintf("$t'=%d$", prev.end)),
      size=text.size,
      hjust=0,
      vjust=-0.1,
      data=data.table(
        chosen.loss, type="total", panel=pfac("data values")))+
    geom_label(aes(
      prev.end+0.5,
      value,
      color=type,
      label=sprintf(
        "optimal $t^*=%d$",
        prev.end)),
      vjust=0.5, hjust=1,
      label.size=0,
      alpha=0.5,
      size=text.size,
      data=data.table(panel=pfac("candidate loss"), loss.min))+
    geom_label(aes(
      prev.end+0.5,
      value,
      color=type,
      label=sprintf(
        "$\\mathcal L_{%d,%d}=%.2f$",
        n.segs, up.to, value)),
      vjust=0.5, hjust=0,
      label.size=0,
      alpha=0.5,
      size=text.size,
      data=data.table(panel=pfac("candidate loss"), loss.min))+
    geom_point(aes(
      prev.end+0.5,
      value,
      color=type),
      data=data.table(panel=pfac("candidate loss"), loss.min))+
    facet_grid(panel ~ ., scales="free")
  gg
}
```

---

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(1)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(41)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(100)
```

--- 

# Initialization of dynamic programming

```{r, dev='tikz', results="asis"}
loss1(nrow(one.dt))
```

---

# Efficient computation of c values

The best square loss up to $t$ is
$$
c_{(0, t]} = \min_u \sum_{i=1}^t (z_i-u)^2 =
\sum_{i=1}^t z_i^2 - \min_u 2\hat u\sum_{i=1}^t z_i + t\hat u^2.
$$

We can use cumulative sum to compute $S_t$ for all $t$ in linear
$O(t)$ time,
$S_t = \sum_{i=1}^t z_i.$

And best means up to $t$,
$\hat u_t = S_t / t.$

And cumulative sum of squares,
$Q_t = \sum_{i=1}^t z_i^2.$

And best loss up to $t$,
$$ 
\sum_{i=1}^t z_i^2 - 2\hat u_t\sum_{i=1}^t z_i + t\hat u_t^2 =
Q_t - 2(S_t/t)(S_t) + t (S_t/t)^2 = 
Q_t-S_t^2/t.
$$

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(113, 2, 10)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(113, 2, 41)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(113, 2, 80)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 10)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 41)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 80)
```

---

# First iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 2, 150)
```

---

# Time complexity for $S$ segments and $n$ data

```{=latex}
\begin{tabular}{ccccc}
changes & naive & DP(classic) & DP(pruned) & BinSeg \\
\hline
1 & $O(n)$   & $O(n)$   & $O(n)$--$O(n^2)$ & $O(n)$ \\
2 & $O(n^2)$ & $O(n^2)$ & $O(n)$--$O(n^2)$ & $O(\log n)$--$O(n)$\\
3 & $O(n^3)$ & $O(n^2)$ & $O(n)$--$O(n^2)$ & $O(\log n)$--$O(n)$\\
4 & $O(n^4)$ & $O(n^2)$ & $O(n)$--$O(n^2)$ & $O(\log n)$--$O(n)$\\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ 
\end{tabular}
```

Dynamic programming with either functional or inequality pruning leads
to asymptotic speedups, see Maidstone R, Hocking TD, Rigaill G,
Fearnhead P. On optimal multiple changepoint algorithms for large
data. Statistics and Computing (2016).

Space: DP stores $O(n S)$ loss $\mathcal L_{s,t}$ values (always),
BinSeg stores $O(S)$ split candidates (worst case).

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(157, 3, 20)
```

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(157, 3, 113)
```

---

# Second iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(157, 3, 130)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 50)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 100)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 157)
```

---

# Third iteration/changepoint of dynamic programming

```{r, dev='tikz', results="asis"}
showChoice(234, 4, 200)
```

---

# Comparison with previous approximate/heuristic algorithms

Every other algorithm we have seen so far has been
approximate/heuristic, because it is not guaranteed to compute a
global optimum of the objective function.

- K-means: local minimum squared error.
- EM algo for Gaussian mixtures: local maximum likelihood.
- Binary segmentation: local minimum squared error
  (maximum Gaussian likelihood).

In contrast the dynamic programming algorithm is guaranteed to compute
a solution (segment means and changepoints) which is globally optimal
(maximum Gaussian likelihoood, minimum squared error).

Dynamic programming is $O(n^2 S)$ for $S$ segments and $n$ data,
whereas binary segmentation is $O(n \log S)$ best case and $O(n S)$
worst case. (optimal algorithms are typically slower than
approximate/heuristic algorithms)

---

# Other optimal algorithms

So far we have only discussed the *constrained problem*:
hyper-parameter is number of segments $S\in\{1,\dots,n\}$.

An alternative formulation involves specifying a non-negative penalty
hyper-parameter, $\lambda\in\mathbb R^+$, and then solving the
*penalized problem*:

$$
\min_{m_1,\dots,m_n\in\mathbb R}
\underbrace{
\sum_{i=1}^n
\ell(m_i, z_i)
}_{
\text{loss}
}
+
\lambda
\underbrace{
\sum_{i=1}^{n-1}
I[m_i \neq m_{i+1}].
}_{
\text{number of changes}
}
$$

The resulting model is the solution to a constrained problem (for some
number of segments $S$). 

Dynamic programming can also be used to solve this problem, using
$O(n)$ space, best case $O(n)$ time (PELT algorithm), worst case
$O(n^2)$ time. Optimal Detection of Changepoints With a Linear
Computational Cost Killick R, Fearnhead P and Eckley IA. Journal of
the American Statistical Association (2012).

For large model sizes $S$ the penalized problem is much faster to
solve than the constrained problem.

---

# Possible exam questions

- For $S$ segments how does the binary segmentation loss compare to
  the dynamic programming loss? Answer for each $S\in\{1,2,3,4\}$ one
  or more of: less than, equal, and/or greater than.
- How many values for the last changepoint variable are considered in
  the computation of $\mathcal L_{3,100}$ ?
- Is {binary segmentation, dynamic programming} an {optimal,
  approximate} algorithm? Why?
