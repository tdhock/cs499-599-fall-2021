---
title: "Hidden Markov Models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=7)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
nb.dt[, data.i := rank(position), keyby=.(profile.id, chromosome)]
chrom.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position, logratio),
    data=chrom.dt)
```

---

# Motivation for Hidden Markov Models (HMMs)

- Sometimes we have an interpretation / expectation of what the
  segments/clusters mean.
- For example in DNA copy number data the logratio=0 means normal copy
  number (two copies -- one from each parent), whereas higher logratio
  values indicate gain/amplification and lower values indicate
  loss/deletion.
  
---

# HMM ideas

- Each observed data variable in the sequence has a corresponding
  un-observed (hidden) state variable.
- There are typically a finite number of possible values for each
  hidden state variable, $k\in\{1,\dots,K\}$.
- Markov assumption: first-order dependency (each hidden variable only
  depends on the previous hidden variable in the sequence).
  
```{r fig.height=3}
N.vars <- 5
var.dt <- data.table::CJ(
  type=c("observed", "hidden"), 
  data.i=1:N.vars)
var.dt[, y := ifelse(type == "observed", 0, 1)]
var.dt[, known := type == "observed"]
arrow.dt <- rbind(
  data.table(
    x=seq(1, N.vars-1),
    xend=seq(2, N.vars)-0.2,
    y=1,
    yend=1),
  data.table(
    x=1:N.vars,
    xend=1:N.vars,
    y=1, 
    yend=0.2))
gg <- ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    arrow=grid::arrow(length=grid::unit(1, "lines")),
    data=arrow.dt)+
  geom_label(aes(
    data.i, y,
    fill=known,
    label=paste0(type, "\n", data.i)),
    data=var.dt)+
  scale_fill_manual(values=c("TRUE"="white", "FALSE"="grey"))+
  scale_x_continuous(
    "Index in data sequence")+
  scale_y_continuous(
    "",
    limits=c(-0.2, 1.2),
    breaks=NULL)
gg

```

---

# Parameters of HMM

- transition matrix: $A\in\mathbb [0,1]^{K\times K}$ for K
  clusters. Each entry $a_{ij}$ is the probability of transitioning
  from state $i$ to state $j$.
- initial state distribution: $\pi\in[0,1]^K$ (prior weights). 
- emission: likelihood of observing data $y$ in state $k$,
  $b_k(y)=\text{NormalDensity}(y, \mu_k, \sigma^2)\in\mathbb R$,
  parameterized by mean $\mu_k$, variance $\sigma^2$ (or
  $\text{standard deviation} = \text{sd}=\sigma$).
- These parameters are unknown in advance and must be learned from the
  data.
- Comparison with Gaussian Mixture Models: HMM has all of GMM
  parameters, plus transition matrix.
  
```{r, fig.height=3}
param.dt <- rbind(
  data.table(
    x=seq(2, N.vars)-0.6,
    y=1,
    angle=0,
    label="transition"),
  data.table(
    x=seq(1, N.vars),
    y=0.6,
    angle=90,
    label="emission"),
  data.table(
    x=0.5, y=1, angle=0, label="initial"))
param.dt[, known := FALSE]
gg+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    arrow=grid::arrow(length=grid::unit(1, "lines")),
    data=data.table(x=0.5, xend=0.8, y=1, yend=1))+
  geom_label(aes(
    x, y, label=label, angle=angle, fill=known),
    vjust=1,
    data=param.dt)
```

---

# Three problems and algorithms

```{r}
## https://web.stanford.edu/~jurafsky/slp3/A.pdf
```

- Evaluation of likelihood of a given data sequence and model
  parameters => forward algorithm.

- Decoding most likely sequence of hidden states given observed data
  => Viterbi algorithm.

- Learning model parameters that maximize likelihood of a given data
  set => Baum-Welch algorithm.
  
---

# Forward algorithm

- Assume each state $k\in\{1,\dots,K\}$ has an emission probability
  function $b_k(o_t)$ for observed data $o_t$ at time $t$.
- Let $a_{kj}$ be a transition parameter (probability of going from
  state $k$ to state $j$).
- Initialization: for all states $j\in\{1,\dots,K\}$, $\alpha_1(j) =
  \pi_j b_j(o_1)$.
- Then we can recursively compute the forward path probability,
  $\alpha_t(j) = \sum_{k=1}^K \alpha_{t-1}(k) a_{kj} b_j(o_t).$
- Total log likelihood is summed over all states at the last data
  point in the sequence.

```{r, fig.height=3.5}
mean.vec <- c(0.25, 0, -0.5)
n.states <- length(mean.vec)
y.vec <- chrom.dt[["logratio"]]
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
sd.param <- 1
## Need to compute emission whenever parameters are updated.
N.data <- length(y.vec)
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec <- log(rep(1/n.states, n.states))
fwd.list <- plotHMM::forward_interface(
  log.emission.mat, log.A.mat, log.pi.vec)
tile.dt <- with(fwd.list, data.table(
  data.i=as.integer(row(log_alpha)),
  state=as.integer(col(log_alpha)),
  log.prob=as.numeric(log_alpha)))
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(chrom.dt, ypanel="data values"))+
  geom_tile(aes(
    data.i, state, fill=log.prob),
    color="black",
    data=data.table(tile.dt, ypanel="forward probability"))+
  scale_fill_gradient(low="white", high="red")+
  facet_grid(ypanel ~ ., scales="free")

```

---

# The numerical underflow problem

- Probability $p_t$ at each time $t$ is in [0,1] so $\prod_{t=1}^N
  p_t\rightarrow 0$.
- For large enough $N$ (several hundred) double precision arithmetic
  underflows (probability = 0 on computer although it is non-zero
  mathematically).

```{r, echo=TRUE, results=TRUE}
N.data <- seq(321, 325)
prob <- 0.1^N.data
data.table(N.data, prob, log.prob=log10(prob))
```

---

# Implementation details to avoid numerical underflow

- Use a double precision number to store log(probability) value
  instead of probability value.
- Instead of multiplying probability values, sum log(probability)
  values.

```{r, echo=TRUE, results=TRUE}
N.data <- seq(321, 325)
log.prob <- N.data*log10(0.1)
data.table(N.data, log.prob, prob=10^log.prob)
```

# Implementation details to avoid numerical underflow

Instead of summing probability values, use log-sum-exp trick (subtract
away $m=\max\{\log p, \log q\}$ in exponent), $p + q = e^{\log p} +
e^{\log q}$.
$$\log(p+q) = \log(e^{\log p} + e^{\log q}) = m + \log(e^{\log p-m}-e^{\log q-m})$$

```{r}
options(warn=1)
```

```{r, echo=TRUE, results=TRUE}
data.table(log.p=-324, log.q=seq(-320, -326)
)[,         m := ifelse(log.p < log.q, log.q, log.p)][,
  log.sum.exp := m + log10(10^(log.p-m)+10^(log.q-m))][,
    naive.sum := log10(10^log.p+10^log.q)][]
```

---

# Viterbi algorithm

- Assume each state $k\in\{1,\dots,K\}$ has an emission probability
  function $b_k(o_t)$ for observed data $o_t$ at time $t$.
- Let $a_{kj}$ be a transition parameter (probability of going from
  state $k$ to state $j$).
- Initialization: for all states $j\in\{1,\dots,K\}$, $v_1(j) = \pi_j b_j(o_1)$.
- Then we can recursively compute the probability of the best sequence
  of hidden variables that ends at data point $t$ in state $j$,
  $v_t(j) = \max_{k\in\{1,\dots,K\}} v_{t-1}(k) a_{kj} b_j(o_t).$
- Also need to store a matrix of best $k$ values which achieved the
  max for every $t,j$.
- To compute best state sequence, first find $k$ with max $v_N(k)$
  then repeatedly examine previously stored best $k$ values.
  
---

# Viterbi algorithm example

```{r fig.height=5, results=TRUE}
mean.vec <- c(0.25, 0, -0.5)
n.states <- length(mean.vec)
y.vec <- chrom.dt[seq(1, .N, by=20), logratio]
A.mat <- matrix(0.1, n.states, n.states)
diag(A.mat) <- 0.8
##A.mat
log.A.mat <- log(A.mat)
sd.param <- 0.1
## Need to compute emission whenever parameters are updated.
N.data <- length(y.vec)
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec <- log(rep(1/n.states, n.states))
str(list(
  mean=mean.vec,
  sd=sd.param,
  transition=exp(log.A.mat),
  initial=exp(log.pi.vec)))
viterbi.list <- plotHMM::viterbi_interface(
  log.emission.mat, log.A.mat, log.pi.vec)
some.dt <- data.table(
  data.i=seq_along(y.vec),
  logratio=y.vec,
  best_state_seq = factor(viterbi.list$state_seq))
state.values <- c("#1B9E77", "#D95F02", "#7570B3")
max.prob.dt <- with(viterbi.list, data.table(
    data.i=as.integer(row(log_max_prob)),
    state=as.integer(col(log_max_prob)),
    log.prob=as.numeric(log_max_prob)))
best.prev.dt <- with(viterbi.list, data.table(
    data.i=as.integer(row(best_state)),
    state=as.integer(col(best_state)),
    best_prev=as.numeric(best_state)))
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "")+
  scale_color_manual(values=state.values)+
  geom_point(aes(
    data.i, logratio, color=best_state_seq),
    data=data.table(some.dt, ypanel="data values"))+
  geom_tile(aes(
    data.i, state, fill=log.prob),
    color="black",
    data=data.table(max.prob.dt, ypanel="best prev\nstate/prob"))+
  geom_text(aes(
    data.i, state, label=best_prev),
    data=data.table(best.prev.dt, ypanel="best prev\nstate/prob"))+
  scale_fill_gradient(low="white", high="red")+
  facet_grid(ypanel ~ ., scales="free")
```

---

# Baum-Welch learning algorithm

- Is an instance of Expectation-Maximization (EM), like the Gaussian
  Mixture Model learning algorithm.
- E step involves forward/backward passes over data sequences, to
  compute probability of each data point in each state.
- M step involves re-computing model parameters.
- Repeat until the log likelihood stops increasing.

```{r, fig.height=4}
n.states <- 3
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
mean.vec <- c(-1, 0, 1)*0.1
y.vec <- chrom.dt[["logratio"]]
N.data <- length(y.vec)
sd.param <- 1
## Need to compute emission whenever parameters are updated.
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec <- log(rep(1/n.states, n.states))
prob.dt.list <- list()
lik.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
viterbi.dt.list <- list()
prior.dt.list <- list()
done <- FALSE
prev.log.lik <- -Inf
iteration <- 1
while(!done){
  fwd.list <- plotHMM::forward_interface(
    log.emission.mat, log.A.mat, log.pi.vec)
  log.alpha.mat <- fwd.list[["log_alpha"]]
  log.beta.mat <- plotHMM::backward_interface(log.emission.mat, log.A.mat)
  log.gamma.mat <- plotHMM::multiply_interface(log.alpha.mat, log.beta.mat)
  prob.mat <- exp(log.gamma.mat)
  log.xi.array <- plotHMM::pairwise_interface(
    log.emission.mat, log.A.mat, log.alpha.mat, log.beta.mat)
  ## update rules.
  (log.pi.vec <- log.gamma.mat[1,])
  (mean.vec <- colSums(y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- y.vec-matrix(mean.vec, N.data, n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  log.A.mat <- plotHMM::transition_interface(
    log.gamma.mat[-N.data,], log.xi.array)
  ## M step done, now store stuff to plot.
  log.emission.mat <- dnorm( #need to compute after M step for viterbi.
    y.vec, 
    matrix(mean.vec, N.data, n.states, byrow=TRUE),
    sd.param,
    log=TRUE)
  viterbi.result <- plotHMM::viterbi_interface(
    log.emission.mat, log.A.mat, log.pi.vec)
  viterbi.dt.list[[iteration]] <- data.table(
    iteration,
    data.i=1:N.data,
    state=factor(viterbi.result[["state_seq"]]))
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(exp(log.A.mat)),
    from.state=as.integer(row(log.A.mat)),
    to.state=as.integer(col(log.A.mat)))
  prior.dt.list[[iteration]] <- data.table(
    iteration,
    state=1:n.states,
    prob=exp(log.pi.vec))
  log.lik <- fwd.list[["log_lik"]]
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik)
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    data.i=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
  iteration <- iteration+1
  if(log.lik <= prev.log.lik){
    done <- TRUE
  }
  prev.log.lik <- log.lik
}
viterbi.dt <- do.call(rbind, viterbi.dt.list)
prob.dt <- do.call(rbind, prob.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
trans.dt <- do.call(rbind, trans.dt.list)
prior.dt <- do.call(rbind, prior.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
expand <- diff(range(y.vec))/10
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)
baum.welch.one <- function(it){
  max.dens <- max(dens.dt$density)
  tile.x.vec <- seq(0, max.dens, l=n.states+2)
  names(tile.x.vec) <- seq(-1, n.states)
  state.y <- seq(1, 0, l=n.states+2)[-c(1, n.states+2)]
  it.viterbi <- viterbi.dt[iteration==it]
  it.mean <- mean.dt[iteration==it]
  it.trans <- trans.dt[iteration==it]
  it.prior <- prior.dt[iteration==it]
  it.lik <- lik.dt[iteration==it]
  it.trans.prior <- rbind(
    it.trans,
    it.prior[, .(iteration, prob, from.state=state, to.state=-1)])
  it.trans.prior[, x := tile.x.vec[paste(to.state)] ]
  it.trans.prior[, y := state.y[from.state] ]
  it.dens <- dens.dt[iteration==it]
  it.prob <- prob.dt[iteration==it]
  state.values <- c("#1B9E77", "#D95F02", "#7570B3")
  chrom.dt[, state := it.viterbi[["state"]] ]
  ggplot()+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))+
    xlab("")+
    ylab("")+
    geom_rect(aes(
      xmin=-Inf, xmax=Inf,
      ymin=mean-sd, ymax=mean+sd),
      size=2,
      alpha=0.4,
      color="transparent",
      fill="grey50",
      data=data.table(ypanel="data values", it.mean))+
    geom_hline(aes(
      yintercept=mean, color=state),
      size=1,
      data=data.table(ypanel="data values", it.mean))+
    geom_text(aes(
      Inf, Inf, label=sprintf("sd=%.4f", sd)),
      vjust=1.1,
      hjust=1,
      data=data.table(ypanel="data values", xpanel="density", it.mean[1]))+
    geom_blank(aes(
      density, y),
      data=data.table(xpanel="density", ypanel="data values", dens.dt))+
    geom_tile(aes(
      x, y, fill=prob),
      data=data.table(xpanel="density", ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=sprintf("%.3f", prob)),
      data=data.table(xpanel="density", ypanel="probability", it.trans.prior))+
    geom_text(aes(
      0, y, label="emission"),
      hjust=0,
      vjust=0,
      data=data.table(
        ypanel="data values", xpanel="density",
        y=max(y.grid)))+
    geom_text(aes(
      x, y, label=state, color=state),
      data=data.table(
        xpanel="density", ypanel="probability",
        state=factor(1:n.states),
        x=tile.x.vec[2],
        y=state.y
      ))+
    geom_text(aes(
      x, y, label=type, hjust=hjust),
      vjust=1,
      data=data.table(
        xpanel="density", ypanel="probability",
        type=c("initial", "transition"),
        x=tile.x.vec[c(1, n.states+2)],
        hjust=c(0,1),
        y=1
      ))+
    geom_path(aes(
      density, y, color=state),
      data=data.table(xpanel="density", ypanel="data values", it.dens))+
    geom_label(aes(
      0, mean, color=state, label=state),
      hjust=1,
      data=data.table(xpanel="density", ypanel="data values", it.mean))+
    scale_color_manual(values=state.values)+
    scale_fill_gradient(low="white", high="red", limits=c(0,1))+
    geom_point(aes(
      data.i, logratio, color=state),
      data=data.table(xpanel="data.i", ypanel="data values", chrom.dt))+
    geom_text(aes(
      Inf, Inf, label=sprintf(
        "iteration=%d log(lik)=%.4f", it, log.lik)),
      hjust=1,
      vjust=1.1,
      data=data.table(xpanel="data.i", ypanel="data values", it.lik))+
    geom_line(aes(
      data.i, prob, color=state),
      data=data.table(xpanel="data.i", ypanel="probability", it.prob))+
    geom_blank(aes(
      data.i, prob),
      data=data.table(
        xpanel="data.i", ypanel="probability", data.i=0, prob=c(0,1)))+
    facet_grid(ypanel ~ xpanel, scales="free")
}
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(1)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(10)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(20)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(30)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(35)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(40)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(45)
```

---

# Shared states between chromosomes on a profile?

- Sometimes there are several data sequences which are assumed to have
  the same set of hidden states.
- In this case Baum-Welch can be used to fit HMM to all data at the
  same time (total log likelihood is summed over all data sequences).

```{r, fig.height=5}
pro.dt <- nb.dt[profile.id=="4" & chromosome %in% paste(1:10)]
ggplot()+
  facet_grid(. ~ chromosome, scales="free", space="free")+
  theme_bw()+
  theme(panel.spacing=grid::unit(0, "lines"))+
  scale_x_continuous(
    "Position/index in data sequence",
    breaks=seq(0, 1000, by=100))+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    data.i, logratio),
    data=pro.dt)
```

---

# HMM learned on whole profile

```{r}
pro.dt[, row := 1:.N]
all.y.vec <- pro.dt[["logratio"]]
data.list <- split(pro.dt, paste(pro.dt[["chromosome"]]))
first.row.vec <- pro.dt[data.i==1, row]
last.row.vec <- pro.dt[, .SD[data.i==.N], by=chromosome][["row"]]
n.states <- 4
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
set.seed(1)
mean.vec <- rnorm(n.states)
sd.param <- 1
log.pi.vec <- log(rep(1/n.states, n.states))
prob.dt.list <- list()
lik.dt.list <- list()
prior.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
viterbi.dt.list <- list()
done <- FALSE
prev.log.lik <- -Inf
iteration <- 1
options(warn=2)
while(!done){
  log.lik.vec <- rep(NA, length(data.list))
  all.log.gamma.mat <- matrix(NA, nrow(pro.dt), n.states)
  all.log.xi.array <- array(NA, c(n.states, n.states, nrow(pro.dt)))
  for(chrom.i in seq_along(data.list)){
    one.chrom <- data.list[[chrom.i]]
    row.vec <- one.chrom[["row"]]
    y.vec <- one.chrom[["logratio"]]
    N.data <- length(y.vec)
    log.emission.mat <- dnorm(
      matrix(y.vec, N.data, n.states, byrow=FALSE),
      matrix(mean.vec, N.data, n.states, byrow=TRUE),
      sd.param,
      log=TRUE)
    fwd.list <- plotHMM::forward_interface(
      log.emission.mat, log.A.mat, log.pi.vec)
    log.alpha.mat <- fwd.list[["log_alpha"]]
    log.lik.vec[[chrom.i]] <- fwd.list[["log_lik"]]
    log.beta.mat <- plotHMM::backward_interface(log.emission.mat, log.A.mat)
    all.log.gamma.mat[row.vec, ] <- plotHMM::multiply_interface(
      log.alpha.mat, log.beta.mat)
    all.log.xi.array[,, row.vec[-N.data] ] <- plotHMM::pairwise_interface(
      log.emission.mat, log.A.mat, log.alpha.mat, log.beta.mat)
    viterbi.result <- plotHMM::viterbi_interface(
      log.emission.mat, log.A.mat, log.pi.vec)
    viterbi.dt.list[[paste(iteration, chrom.i)]] <- data.table(
      iteration, chrom.i,
      one.chrom, 
      state=factor(viterbi.result[["state_seq"]]))
  }
  ## update rules.
  (log.pi.vec <- apply(
    all.log.gamma.mat[first.row.vec,]-log(length(first.row.vec)),
    2, plotHMM::logsumexp))
  prob.mat <- exp(all.log.gamma.mat)
  (mean.vec <- colSums(all.y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- all.y.vec-matrix(
    mean.vec, length(all.y.vec), n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  log.A.mat <- plotHMM::transition_interface(
    all.log.gamma.mat[-last.row.vec,],
    all.log.xi.array[,, -last.row.vec])
  prior.dt.list[[iteration]] <- data.table(
    iteration,
    state=1:n.states,
    prob=exp(log.pi.vec))
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(exp(log.A.mat)),
    from.state=as.integer(row(log.A.mat)),
    to.state=as.integer(col(log.A.mat)))
  log.lik <- sum(log.lik.vec)
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik)
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    row=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
  cat(sprintf("iteration=%4d log.lik=%f\n", iteration, log.lik))
  iteration <- iteration+1
  if(log.lik <= prev.log.lik){
    done <- TRUE
  }
  prev.log.lik <- log.lik
}
viterbi.dt <- do.call(rbind, viterbi.dt.list)
prob.dt <- do.call(rbind, prob.dt.list)
prior.dt <- do.call(rbind, prior.dt.list)
trans.dt <- do.call(rbind, trans.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
y.vec <- pro.dt$logratio
expand <- diff(range(y.vec))/10
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
dens.dt[, norm.density := max(pro.dt$data.i)*density/max(density)]
max.dens <- max(dens.dt$norm.density)
ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)
## for debugging https://iulg.sitehost.iu.edu/moss/hmmcalculations.pdf
baum.welch.multiple <- function(it){
  tile.x.vec <- seq(0, max.dens, l=n.states+2)
  names(tile.x.vec) <- seq(-1, n.states)
  state.y <- seq(1, 0, l=n.states+2)[-c(1, n.states+2)]
  it.trans <- trans.dt[iteration==it]
  it.prior <- prior.dt[iteration==it]
  it.dens <- dens.dt[iteration==it]
  it.trans.prior <- rbind(
    it.trans,
    it.prior[, .(iteration, prob, from.state=state, to.state=-1)])
  it.trans.prior[, x := tile.x.vec[paste(to.state)] ]
  it.trans.prior[, y := state.y[from.state] ]
  it.lik <- lik.dt[iteration==it]
  it.mean <- mean.dt[iteration==it]
  it.viterbi <- viterbi.dt[iteration==it]
  it.prob <- prob.dt[iteration==it][pro.dt, on="row"]
  state.values <- c(
    "#E7298A","#7570B3",
    "#1B9E77", "#D95F02")
  cfac <- function(x)factor(x, c(1:10,"density"))
  ggplot()+
    scale_x_continuous(
      "Position/index in data sequence",
      breaks=seq(0, 1000, by=100)[-1])+
    ylab("")+
    scale_color_manual(values=state.values)+
    geom_text(aes(
      Inf, Inf, label=sprintf(
        "iteration=%d\nlog(lik)=%.4f", it, log.lik)),
      hjust=1,
      vjust=1.1,
      data=data.table(
        it.lik,
        ypanel="data values",
        chromosome=cfac("1")))+
    geom_rect(aes(
      xmin=-Inf, xmax=Inf,
      ymin=mean-sd, ymax=mean+sd),
      size=2,
      alpha=0.4,
      color="transparent",
      fill="grey50",
      data=data.table(ypanel="data values", it.mean))+
    geom_hline(aes(
      yintercept=mean, color=state),
      size=1,
      data=data.table(ypanel="data values", it.mean))+
    geom_point(aes(
      data.i, logratio, color=state),
      data=data.table(ypanel="data values", it.viterbi))+
    geom_path(aes(
      norm.density, y, color=state),
      data=data.table(ypanel="data values", chromosome=cfac("density"), it.dens))+
    geom_text(aes(
      0, y, label="emission"),
      hjust=0,
      vjust=0,
      data=data.table(
        ypanel="data values", chromosome=cfac("density"),
        y=max(y.grid)))+
    geom_blank(aes(
      norm.density, y),
      data=data.table(
        ypanel="data values", chromosome=cfac("density"), dens.dt))+
    geom_label(aes(
      0, mean, color=state, label=state),
      hjust=1,
      data=data.table(
        chromosome=cfac("density"), ypanel="data values", it.mean))+
    geom_tile(aes(
      x, y, fill=prob),
      data=data.table(
        chromosome=cfac("density"), ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=sprintf("%.1f", prob)),
      data=data.table(
        chromosome=cfac("density"), ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=state, color=state),
      data=data.table(
        chromosome=cfac("density"), ypanel="probability",
        state=factor(1:n.states),
        x=tile.x.vec[2],
        y=state.y
      ))+
    geom_text(aes(
      x, y, label=type, hjust=hjust),
      vjust=1,
      data=data.table(
        chromosome=cfac("density"), ypanel="probability",
        type=c("initial", "transition"),
        x=tile.x.vec[c(1, n.states+2)],
        hjust=c(0,1),
        y=1
      ))+
    geom_text(aes(
      Inf, Inf, label=sprintf("sd=%.4f", sd)),
      vjust=1.1,
      hjust=1,
      data=data.table(
        ypanel="data values", chromosome=cfac("density"), it.mean[1]))+
    scale_fill_gradient(low="white", high="red", limits=c(0,1))+
    geom_line(aes(
      data.i, prob, color=state),
      data=data.table(ypanel="probability", it.prob))+
    geom_blank(aes(
      data.i, prob),
      data=data.table(ypanel="probability", data.i=0, prob=c(0,1)))+
    facet_grid(ypanel ~ chromosome, scales="free", space="free_x")+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))
}
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(1)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(5)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(10)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(15)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(20)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(25)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(30)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(35)
```

---

# Time/space complexity 

For $K$ states and $N$ data

- Forward/Viterbi: $O(K^2 N)$ time, $O(K N)$ space.
- Each iteration of Baum-Welch: $O(K^2 N)$ time and space.

---

# Comparison with other algorithms

- K-means and Gaussian mixture models also have cluster-specific
  parameters (mean, covariance, prior weight), but are not able to
  model sequential dependence.
- Binary/optimal segmentation require specification of number of
  segments/changepoints rather than number of hidden states.
- All segmentation models we studied had $K$ mean parameters, and a
  single variance/sd parameter common to all segments.
- Binary/optimal segmentation always jump to a new mean parameter,
  whereas HMM may jump to a previously visited mean parameter.
- Binary/optimal segmentation with $K$ segments can be interpreted as
  an HMM with a constrained transition matrix ($a_{ij}=1$ if $j=i+1$
  else 0: always jump to the next state, never jump back to a previous
  state).
- Binary/optimal segmentation log likelihood only uses emission
  probabilities, whereas HMM also includes initial/transition
  probabilities.

---

# Possible exam questions

- In the previous slides we saw $K=3$ or 4 clusters. How many
  parameters of each type are there to learn in each case? (assume
  common sd parameter as in slides)
- How many parameters if each cluster has its own sd parameter?
- In the previous slides the data have a single feature
  (logratio). How many parameters if there are $P=2$ real-valued
  features instead? (assume normal distribution with no constraints on
  covariance matrix)
