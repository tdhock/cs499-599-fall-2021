---
title: "Hidden Markov Models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
setkey(nb.dt, profile.id, chromosome)
one.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position, logratio),
    data=one.dt)
```

---

# Motivation for Hidden Markov Models (HMMs)

- Sometimes we have an interpretation / expectation of what the
  segments/clusters mean.
- For example in DNA copy number data the logratio=0 means normal copy
  number (two copies -- one from each parent), whereas higher logratio
  values indicate gain/amplification and lower values indicate
  loss/deletion.
- HMMs have a hidden state variable which is associated with each
  observed data variable in the sequence.
- There are typically a finite number of hidden states/clusters,
  $k\in\{1,\dots,K\}$.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some emission distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.
- These parameters are unknown in advance and must be learned from the
  data.
- Comparison with Gaussian Mixture Models: HMM has all of GMM
  parameters, plus transition matrix.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.

---

# Computations

```{r}
n.states <- 3
A.mat <- matrix(1/n.states, n.states, n.states)
mean.vec <- c(-1, 0, 1)*0.1
sd.param <- 1
pi.vec <- rep(1/n.states, n.states)
y.vec <- one.dt[["logratio"]]
N.data <- length(y.vec)
alpha.mat <- matrix(NA, N.data, n.states)
log.emission.mat <- dnorm(
  matrix(y.vec, N.data, n.states, byrow=FALSE),
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
alpha.mat[1,] <- pi.vec * exp(log.emission.mat[1,])
for(data.t in 2:N.data){
  alpha.mat[data.t,] <- exp(log.emission.mat[data.t,])*
    colSums(A.mat*matrix(alpha.mat[data.t-1,], n.states, n.states))
}
mlapp.alpha.mat <- matrix(NA, N.data, n.states)
prob.vec <- pi.vec * exp(log.emission.mat[1,])
Z <- sum(prob.vec)
mlapp.alpha.mat[1,] <- prob.vec/Z
Z.vec <- rep(NA, N.data)
Z.vec[1] <- Z
for(data.t in 2:N.data){
  emission.vec <- exp(log.emission.mat[data.t,])
  prob.vec <- emission.vec * t(A.mat) %*% mlapp.alpha.mat[data.t-1,]
  Z <- sum(prob.vec)
  mlapp.alpha.mat[data.t,] <- prob.vec/Z
  Z.vec[data.t] <- Z
}
## MLAPP equation (17.49) log(x_{1:T}|theta) = sum_{t=1}^T log(Z_t)
sum(log(Z.vec))
##http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf
elnsum <- function(elnx, elny){
  fcase(
    elnx == -Inf, elny,
    elny == -Inf, elnx,
    elny < elnx, elnx + log(1+exp(elny-elnx)),
    elnx <= elny, elny + log(1+exp(elnx-elny)))
}
elnproduct <- function(elnx, elny){
  ifelse(
    elnx == -Inf | elny == -Inf,
    -Inf,
    elnx+elny)
}
logsumexp <- function(exponents.vec){
  m <- max(exponents.vec)
  m + log(sum(exp(exponents.vec-m)))
}
log.alpha.mat <- matrix(NA, N.data, n.states)
log.alpha.mat[1,] <- elnproduct(
  log(pi.vec),
  log.emission.mat[1,])
for(data.t in 2:N.data){
  log.alpha.vec <- rep(-Inf, n.states)
  for(state.i in 1:n.states){
    prod.vec <- elnproduct(
      log.alpha.mat[data.t-1,state.i],
      log(A.mat[state.i,]))
    log.alpha.vec <- elnsum(log.alpha.vec, prod.vec)
  }
  emission.prob.vec <- log.emission.mat[data.t,]
  log.alpha.mat[data.t,] <- elnproduct(log.alpha.vec, emission.prob.vec)
}
diff.alpha.mat <- log.alpha.mat-log(alpha.mat)
head(diff.alpha.mat)
max(abs(diff.alpha.mat))#should be zero
log.beta.mat <- matrix(NA, N.data, n.states)
log.beta.mat[N.data, ] <- 0
for(data.t in seq(N.data-1, 1)){
  log.beta.vec <- rep(-Inf, n.states)
  for(state.j in 1:n.states){
    log.emission.prob <- log.emission.mat[data.t+1,]
    inside.product <- elnproduct(log.emission.prob, log.beta.mat[data.t+1,])
    outside.product <- elnproduct(log(A.mat[,state.j]), inside.product)
    log.beta.vec <- elnsum(log.beta.vec, outside.product)
  }
  log.beta.mat[data.t,] <- log.beta.vec
}
log.gamma.mat <- matrix(NA, N.data, n.states)
normalizer <- rep(-Inf, N.data)
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.alpha.mat[,state.i],log.beta.mat[,state.i])
  normalizer <- elnsum(normalizer, log.gamma.mat[,state.i])
}
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.gamma.mat[,state.i], -normalizer)
}
prob.mat <- exp(log.gamma.mat)
rowSums(prob.mat)#should be one
## array-based?
arr.dim <- c(dim(A.mat), N.data-1)
Aij.array <- array(A.mat, arr.dim)
str(Aij.array)
Aij.array[,,1:2]
A.mat
inner.prod <- elnproduct(log.emission.mat[-1,], log.beta.mat[-1,])
inner.arr <- array(rep(t(inner.prod), each=n.states), arr.dim)
inner.arr[,,1:2]
inner.prod[1:2,]
second.arr <- elnproduct(Aij.array, inner.arr)
second.arr[,,1:2]
log.alpha.arr <- array(
  apply(log.alpha.mat[-N.data,], 1, rep, n.states),
  arr.dim)
log.alpha.arr[,,1:2]
log.alpha.mat[1:2,]
log.xi.arr <- elnproduct(log.alpha.arr, second.arr)
normalizer <- rep(-Inf, N.data-1)
for(state.i in 1:n.states){
  for(state.j in 1:n.states){
    normalizer <- elnsum(normalizer, log.xi.arr[state.i, state.j,])
  }
}
log.xi.norm <- array(NA, arr.dim)
for(state.i in 1:n.states){
  for(state.j in 1:n.states){
    log.xi.norm[state.i, state.j, ] <- elnproduct(
      log.xi.arr[state.i, state.j, ], -normalizer)
  }
}
xi.arr <- array(NA, c(n.states, n.states, N.data-1))
for(data.t in seq(1, N.data-1)){
  normalizer <- -Inf
  for(state.i in 1:n.states){
    for(state.j in 1:n.states){
      first.product <- elnproduct(
        log.emission.mat[data.t+1, state.j],
        log.beta.mat[data.t+1, state.j])
      second.product <- elnproduct(
        first.product,
        log(A.mat[state.i, state.j]))
      value <- elnproduct(
        second.product,
        log.alpha.mat[data.t, state.i])
      normalizer <- elnsum(normalizer, value)
      xi.arr[state.i, state.j, data.t] <- value
    }
  }
  xi.arr[,, data.t] <- elnproduct(xi.arr[,, data.t], -normalizer)
}

## update rules.
(pi.vec <- prob.mat[1,])
(mean.vec <- colSums(y.vec*prob.mat)/colSums(prob.mat))
resid.mat <- y.vec-matrix(mean.vec, N.data, n.states, byrow=TRUE)
sd.param <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
apply(log.gamma.mat, 1, which.max)
## https://web.stanford.edu/~jurafsky/slp3/A.pdf

```

```{r}
## for debugging https://iulg.sitehost.iu.edu/moss/hmmcalculations.pdf
st <- function(...)structure(c(...), names=c("s","t"))
pi.vec <- st(.85, .15)
initial.dist <- rbind(
  A=st(.4, .5),
  B=st(.6, .5))
A.mat <- rbind(
  s=st(.3, .7),
  t=st(.1, .9))
seq1=strsplit("ABBA",split="")[[1]]
N.data <- length(seq1)
log.emission.mat <- log(initial.dist[seq1,])
n.states <- length(pi.vec)
log.alpha.mat <- matrix(NA, N.data, n.states)
log.alpha.mat[1,] <- elnproduct(
  log(pi.vec),
  log.emission.mat[1,])
(log.alpha.mat[1,] <- log(c(.34, .08)))
for(data.t in 2:N.data){
  log.alpha.vec <- rep(-Inf, n.states)
  for(state.i in 1:n.states){
    prod.vec <- elnproduct(
      log.alpha.mat[data.t-1,state.i],
      log(A.mat[state.i,]))
    log.alpha.vec <- elnsum(log.alpha.vec, prod.vec)
  }
  emission.prob.vec <- log.emission.mat[data.t,]
  log.alpha.mat[data.t,] <- elnproduct(log.alpha.vec, emission.prob.vec)
}
alpha.mat <- matrix(NA, N.data, n.states)
alpha.mat[1,] <- pi.vec * exp(log.emission.mat[1,])
alpha.mat[1,] <- c(.34, .08)
for(data.t in 2:N.data){
  prev.mat <- matrix(alpha.mat[data.t-1,], n.states, n.states)
  emission.vec <- exp(log.emission.mat[data.t,])
  emat <- matrix(emission.vec, n.states, n.states, byrow=TRUE)
  alpha.mat[data.t,] <- colSums(A.mat*prev.mat*emat)
}

log.beta.mat <- matrix(NA, N.data, n.states)
log.beta.mat[N.data, ] <- 0
for(data.t in seq(N.data-1, 1)){
  for(state.i in 1:n.states){
    log.beta <- -Inf
    for(state.j in 1:n.states){
      le <- log.emission.mat[data.t+1,state.j]
      lb <- log.beta.mat[data.t+1,state.j]
      la <- log(A.mat[state.i,state.j])
      print(exp(c(la, le, lb)))
      inside.product <- elnproduct(le, lb)
      outside.product <- elnproduct(la, inside.product)
      log.beta <- elnsum(log.beta, outside.product)
    }
    log.beta.mat[data.t,state.i] <- log.beta
  }
}

log.gamma.mat <- matrix(NA, N.data, n.states)
normalizer <- rep(-Inf, N.data)
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.alpha.mat[,state.i],log.beta.mat[,state.i])
  normalizer <- elnsum(normalizer, log.gamma.mat[,state.i])
}
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.gamma.mat[,state.i], -normalizer)
}
prob.mat <- exp(log.gamma.mat)

xi.arr <- array(NA, c(n.states, n.states, N.data-1))
for(data.t in seq(1, N.data-1)){
  normalizer <- -Inf
  for(state.i in 1:n.states){
    for(state.j in 1:n.states){
      first.product <- elnproduct(
        log.emission.mat[data.t+1, state.j],
        log.beta.mat[data.t+1, state.j])
      second.product <- elnproduct(
        first.product,
        log(A.mat[state.i, state.j]))
      value <- elnproduct(
        second.product,
        log.alpha.mat[data.t, state.i])
      normalizer <- elnsum(normalizer, value)
      xi.arr[state.i, state.j, data.t] <- value
    }
  }
  xi.arr[,, data.t] <- elnproduct(xi.arr[,, data.t], -normalizer)
}


```
