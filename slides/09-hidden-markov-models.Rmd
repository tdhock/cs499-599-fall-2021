---
title: "Hidden Markov Models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=7)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
theme_set(
  theme_bw()+
    theme(
      panel.spacing=grid::unit(0, "lines"),
      text=element_text(size=20)))
geom.text.size <- 5
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
nb.dt[, data.i := rank(position), keyby=.(profile.id, chromosome)]
chrom.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    data.i, logratio),
    data=chrom.dt)
```

---

# Motivation for Hidden Markov Models (HMMs)

- Sometimes we have an interpretation / expectation of what the
  segments/clusters mean.
- For example in DNA copy number data the logratio=0 means normal copy
  number (two copies -- one from each parent), whereas higher logratio
  values indicate gain/amplification and lower values indicate
  loss/deletion.
  
---

# HMM ideas

- Each observed data variable in the sequence has a corresponding
  un-observed (hidden) state variable.
- There are typically a finite number of possible values for each
  hidden state variable, $k\in\{1,\dots,K\}$.
- Markov assumption: first-order dependency (each hidden variable only
  depends on the previous hidden variable in the sequence).
  
```{r fig.height=3}
N.vars <- 5
var.dt <- data.table::CJ(
  type=c("observed", "hidden"), 
  data.i=1:N.vars)
var.dt[, y := ifelse(type == "observed", 0, 1)]
var.dt[, known := type == "observed"]
arrow.dt <- rbind(
  data.table(
    x=seq(1, N.vars-1),
    xend=seq(2, N.vars)-0.2,
    y=1,
    yend=1),
  data.table(
    x=1:N.vars,
    xend=1:N.vars,
    y=1, 
    yend=0.2))
gg <- ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    arrow=grid::arrow(length=grid::unit(1, "lines")),
    data=arrow.dt)+
  geom_label(aes(
    data.i, y,
    fill=known,
    label=paste0(type, "\n", data.i)),
    data=var.dt)+
  scale_fill_manual(values=c("TRUE"="white", "FALSE"="grey"))+
  scale_x_continuous(
    "Index in data sequence")+
  scale_y_continuous(
    "",
    limits=c(-0.2, 1.2),
    breaks=NULL)
gg

```

---

# Parameters of HMM

- transition matrix: $A\in\mathbb [0,1]^{K\times K}$ for K
  clusters. Each entry $a_{ij}$ is the probability of transitioning
  from state $i$ to state $j$.
- initial state distribution: $\pi\in[0,1]^K$ (prior weights). 
- emission: likelihood of observing data $y$ in state $k$,
  $b_k(y)=\text{NormalDensity}(y, \mu_k, \sigma^2)\in\mathbb R$,
  parameterized by mean $\mu_k$, variance $\sigma^2$ (or
  $\text{standard deviation} = \text{sd}=\sigma$).
- These parameters are unknown in advance and must be learned from the
  data.
- Comparison with Gaussian Mixture Models: HMM has all of GMM
  parameters, plus transition matrix.
  
```{r, fig.height=3}
param.dt <- rbind(
  data.table(
    x=seq(2, N.vars)-0.6,
    y=1,
    angle=0,
    label="transition"),
  data.table(
    x=seq(1, N.vars),
    y=0.6,
    angle=90,
    label="emission"),
  data.table(
    x=0.5, y=1, angle=0, label="initial"))
param.dt[, known := FALSE]
gg+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    arrow=grid::arrow(length=grid::unit(1, "lines")),
    data=data.table(x=0.5, xend=0.8, y=1, yend=1))+
  geom_label(aes(
    x, y, label=label, angle=angle, fill=known),
    vjust=1,
    data=param.dt)
```

---

# Problems and algorithms

```{r}
## https://web.stanford.edu/~jurafsky/slp3/A.pdf
```

- Evaluation of likelihood of a given data sequence and model
  parameters => forward algorithm.

- Decoding most likely sequence of hidden states given observed data
  and model parameters => Viterbi algorithm.

- Learning model parameters that maximize likelihood of a given data
  set => Baum-Welch algorithm.
  
- Forward-backward sub-routine: computing the likelihood of being in
  each state at each time point, given model parameters and data.
  
- Notation: probability is a value between zero and one; likelihood is
  not necessarily between zero and one; both can be used to measure
  the goodness of fit of a model to data (larger is better).
  
---

# Forward algorithm

- Assume each state $k\in\{1,\dots,K\}$ has an emission likelihood
  function $b_k(o_t)$ for observed data $o_t$ at time $t$.
- Let $a_{kj}$ be a transition parameter (probability of going from
  state $k$ to state $j$).
- Initialization: for all states $j\in\{1,\dots,K\}$, $\alpha_1(j) =
  \pi_j b_j(o_1)$.
- Then we can recursively compute the forward path likelihood,
  $\alpha_t(j) = \sum_{k=1}^K \alpha_{t-1}(k) a_{kj} b_j(o_t).$
- Total log likelihood is summed over all states at the last data
  point in the sequence.
  
```{r, fig.height=3.5}
mean.vec <- c(0.3, 0, -0.5)
n.states <- length(mean.vec)
log.pi.vec <- log(c(0.3, 0.3, 0.4))
log.pi.dt <- data.table(
  data.i=-1,
  state.id=seq(1, n.states),
  state=seq(n.states, 1),
  log.prob=log.pi.vec)
y.vec <- chrom.dt[seq(1, .N, by=20), logratio]
y.dt <- data.table(data.i=seq_along(y.vec), logratio=y.vec)
diag.entry <- 0.6
A.mat <- matrix((1-diag.entry)/2, n.states, n.states)
diag(A.mat) <- diag.entry
log.A.mat <- log(A.mat)
##log.A.mat[i,j] is the log probability of going from state i
##to state j.
state.y <- function(state.id){
  (n.states-state.id)+2+n.states
}
log.A.dt <- data.table(
  from=as.integer(row(log.A.mat)),
  to=as.integer(col(log.A.mat)),
  log.prob=as.numeric(log.A.mat)
)[, `:=`(data.i=to-1-n.states, state=state.y(from))]
sd.param <- 0.1
N.data <- length(y.vec)
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.emission.dt <- data.table(
  data.i=as.integer(row(log.emission.mat)),
  state.id=as.integer(col(log.emission.mat)),
  log.prob=as.numeric(log.emission.mat)
)[, `:=`(state=state.y(state.id))]
str(list(
  mean=mean.vec,
  sd=sd.param,
  transition=exp(log.A.mat),
  initial=exp(log.pi.vec)))
fwd.list <- plotHMM::forward_interface(
  log.emission.mat, log.A.mat, log.pi.vec)
tile.dt <- with(fwd.list, data.table(
  data.i=as.integer(row(log_alpha)),
  state.id=as.integer(col(log_alpha)),
  log.prob=as.numeric(log_alpha)
)[, state := n.states-state.id+1])
ypanel <- "Forward\nalgorithm"
lab <- function(data.i, state, label, hjust, yp=ypanel){
  data.table(data.i, state, label, hjust, ypanel=yp)
}
text.dt <- rbind(
  lab(-1, 4, "Prior param.", 1),
  lab(-2, 8, "Transition param.", 0.5),
  lab(1, 4, "Forward lik.", 0),
  lab(1, 8, "Emission lik.", 0),
  lab(0, c(1:3, 5:7), 3:1, 0.5),
  lab(0, 8, "State", 0.5),
  lab(0, -0.25, sprintf("Emission parameters\nSD=%.1f",sd.param), 1, "data values"))
mean.dt <- data.table(
  state.id=seq_along(mean.vec),
  mean=mean.vec)
gg.fwd <- function(state.k, data.t){
  some.tiles <- rbind(
    log.pi.dt[, .(data.i, state, log.prob)],
    log.emission.dt[, .(data.i, state, log.prob)],
    log.A.dt[, .(data.i, state, log.prob)], 
    tile.dt[
    (state.id <= state.k & data.i == data.t) |
      data.i < data.t, .(data.i, state, log.prob)])
  state.t <- n.states-state.k+1
  seg.dt <- rbind(
    data.table(),
    if(data.t==1)log.emission.dt[
      data.i==data.t & state.id == state.k, data.table(
        data.prev=data.i+0.3,
        data.t,
        state.prev=state,
        state.t)]
    else if(data.t<=length(y.vec))log.emission.dt[
      data.i==data.t, data.table(
        data.prev=data.i+0.3*seq(-1, 1, l=n.states),
        data.t,
        state.prev=state,
        state.t)],
    if(data.t==1)log.pi.dt[state.id==state.k, data.table(
      data.prev=data.i+0.3,
      data.t,
      state.prev=state,
      state.t)]
    else if(data.t<=length(y.vec))log.A.dt[to==state.k, data.table(
      data.prev=data.i+0.3,
      data.t,
      state.prev=state, 
      state.t)],
    if(data.t>1 && data.t <= length(y.vec))data.table(
      data.prev=data.t-1,
      data.t,
      state.prev=1:n.states,
      state.t))
  gg <- ggplot()+
    scale_x_continuous(
      "Position/index in data sequence",
      breaks=tile.dt$data.i)+
    scale_y_continuous(
      "",
      breaks=function(limits){
        if(limits[2]<1)seq(-1, 1, by=0.25)
      })+
    geom_point(aes(
      data.i, logratio),
      data=data.table(y.dt, ypanel="data values"))+
    geom_rect(aes(
      xmin=-Inf, xmax=Inf,
      ymin=mean-sd.param, ymax=mean+sd.param),
      alpha=0.5,
      data=data.table(mean.dt, ypanel="data values"))+
    geom_hline(aes(
      yintercept=mean),
      data=data.table(mean.dt, ypanel="data values"))+
    geom_label(aes(
      0, mean, label=sprintf("mean %d = %.1f", state.id, mean)),
      hjust=1,
      data=data.table(mean.dt, ypanel="data values"))+
    geom_tile(aes(
      data.i, state, fill=log.prob),
      color="black",
      data=data.table(
        some.tiles, 
        ypanel))+
    geom_text(aes(
      data.i, state, label=sprintf("%.2f", exp(log.prob))),
      color="black",
      data=data.table(
        some.tiles, 
        ypanel))+
    geom_text(aes(
      data.i, state, label=label, hjust=hjust),
      data=text.dt)+
    scale_fill_gradient(
      low="white", high="red", 
      limits=range(tile.dt$log.prob, log.emission.mat))+
    facet_grid(ypanel ~ ., scales="free")
  if(nrow(seg.dt)){
    gg <- gg+geom_segment(aes(
      data.t, state.t+0.3,
      xend=data.prev, yend=state.prev),
      data=data.table(
        seg.dt,
        ypanel),
      size=1,
      color="grey")
  }
  gg
}
gg.fwd(3, 13)

```

---

# Forward algo demo

```{r forward-begin}
gg.fwd(1, 1)
```

---

# Forward algo demo

```{r}
gg.fwd(2, 1)
```

---

# Forward algo demo

```{r}
gg.fwd(3, 1)
```

---

# Forward algo demo

```{r}
gg.fwd(1, 2)
```

---

# Forward algo demo

```{r}
gg.fwd(2, 2)
```

---

# Forward algo demo

```{r}
gg.fwd(3, 2)
```

---

# Forward algo demo

```{r}
gg.fwd(1, 3)
```

---

# Forward algo demo

```{r}
gg.fwd(2, 3)
```

---

# Forward algo demo

```{r}
gg.fwd(3, 3)
```

---

# Forward algo demo

```{r}
gg.fwd(1, 11)
```

---

# Forward algo demo

```{r}
gg.fwd(2, 11)
```

---

# Forward algo demo

```{r}
gg.fwd(3, 11)
```

---

# Forward algo demo

```{r}
gg.fwd(1, 12)
```

---

# Forward algo demo

```{r}
gg.fwd(2, 12)
```

---

# Forward algo demo

```{r forward-end}
gg.fwd(3, 12)
```
  
---

# Back to full data, numerical underflow problem

- When running the forward algorithm on large data (more than a few
  hundred time points), the log likelihood gets very small at the end
  of the sequence, which may result in numerical underflow.

```{r, fig.height=3.5}
mean.vec <- c(0.25, 0, -0.5)
n.states <- length(mean.vec)
y.vec <- chrom.dt[["logratio"]]
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
sd.param <- 1
## Need to compute emission whenever parameters are updated.
N.data <- length(y.vec)
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec <- log(rep(1/n.states, n.states))
fwd.list <- plotHMM::forward_interface(
  log.emission.mat, log.A.mat, log.pi.vec)
tile.dt <- with(fwd.list, data.table(
  data.i=as.integer(row(log_alpha)),
  state=as.integer(col(log_alpha)),
  log.prob=as.numeric(log_alpha)))
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(chrom.dt, ypanel="data values"))+
  geom_tile(aes(
    data.i, state, fill=log.prob),
    color="black",
    data=data.table(tile.dt, ypanel="forward\nlikelihood"))+
  scale_fill_gradient(low="white", high="red")+
  facet_grid(ypanel ~ ., scales="free")

```

---

# The numerical underflow problem

- Probability $p_t$ at each time $t$ is in [0,1] so $\prod_{t=1}^N
  p_t\rightarrow 0$.
- For large enough $N$ (several hundred) double precision arithmetic
  underflows (probability = 0 on computer although it is non-zero
  mathematically).

```{r, echo=TRUE, results=TRUE}
N.data <- seq(321, 325)
prob <- 0.1^N.data
data.table(N.data, prob, log.prob=log10(prob))
```

---

# Implementation details to avoid numerical underflow

- Use a double precision number to store log(probability) value, $\log p$,
  instead of probability value, $p$.
- Instead of multiplying probability values, sum log(probability)
  values: $\log(pq) = \log p + \log q$.

```{r, echo=TRUE, results=TRUE}
N.data <- seq(321, 325)
log.prob <- N.data*log10(0.1)
data.table(N.data, log.prob, prob=10^log.prob)
```

# Implementation details to avoid numerical underflow

Instead of summing probability values, use log-sum-exp trick (subtract
away $m=\max\{\log p, \log q\}$ in exponent), $p + q = e^{\log p} +
e^{\log q}$.
$$\log(p+q) = \log(e^{\log p} + e^{\log q}) = m + \log(e^{\log p-m}+e^{\log q-m})$$

```{r}
options(warn=1)
```

```{r, echo=TRUE, results=TRUE}
data.table(log.p=-324, log.q=seq(-320, -326)
)[,         m := ifelse(log.p < log.q, log.q, log.p)][,
  log.sum.exp := m + log10(10^(log.p-m)+10^(log.q-m))][,
    naive.sum := log10(10^log.p+10^log.q)][]
```

---

# Viterbi algorithm

- Assume each state $k\in\{1,\dots,K\}$ has an emission probability
  function $b_k(o_t)$ for observed data $o_t$ at time $t$.
- Let $a_{kj}$ be a transition parameter (probability of going from
  state $k$ to state $j$).
- Initialization: for all states $j\in\{1,\dots,K\}$, $v_1(j) = \pi_j b_j(o_1)$.
- Then we can recursively compute the probability of the best sequence
  of hidden variables that ends at data point $t$ in state $j$,
  $v_t(j) = \max_{k\in\{1,\dots,K\}} v_{t-1}(k) a_{kj} b_j(o_t).$
- Also need to store a matrix of best $k$ values which achieved the
  max for every $t,j$.
- To compute best state sequence, first find $k$ with max $v_N(k)$
  then repeatedly examine previously stored best $k$ values.
  
---

# Viterbi algorithm example for $K=3$ states

```{r fig.height=5, results=TRUE}
mean.vec <- c(-0.5, 0, 0.3)
n.states <- length(mean.vec)
log.pi.vec <- log(c(0.6, 0.2, 0.2))
##log.pi.vec <- log(rep(1/n.states, n.states))
y.vec <- chrom.dt[seq(1, .N, by=20), logratio]
diag.entry <- 0.6
A.mat <- matrix((1-diag.entry)/2, n.states, n.states)
diag(A.mat) <- diag.entry
##A.mat <- matrix(1/n.states, n.states, n.states)
log.A.mat <- log(A.mat)
sd.param <- 0.2
N.data <- length(y.vec)
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
str(list(
  mean=mean.vec,
  sd=sd.param,
  transition=exp(log.A.mat),
  initial=exp(log.pi.vec)))
viterbi.list <- plotHMM::viterbi_interface(
  log.emission.mat, log.A.mat, log.pi.vec)
some.dt <- data.table(
  data.i=seq_along(y.vec),
  logratio=y.vec,
  best_state_seq = factor(viterbi.list$state_seq))
state.values <- c("#1B9E77", "#D95F02", "#7570B3", "white")
max.prob.dt <- with(viterbi.list, data.table(
    data.i=as.integer(row(log_max_prob)),
    state=as.integer(col(log_max_prob)),
    log.prob=as.numeric(log_max_prob)))
best.prev.dt <- with(viterbi.list, data.table(
    data.i=as.integer(row(best_state)),
    state=as.integer(col(best_state)),
    best_prev=as.numeric(best_state))
    )[data.i>1]
ypanel <- "best previous\nstate/probability"
back.segs <- data.table(
  data.t=2:nrow(some.dt),
  state.t=viterbi.list$state_seq[-1],
  data.prev=seq(1, nrow(some.dt)-1),
  state.prev=viterbi.list$state_seq[-nrow(some.dt)])
options(warn=1)
blank.points <- geom_blank(aes(
  data.i, logratio, color=best_state_seq),
  data=data.table(
    data.i=1,logratio=0,best_state_seq=c(1:n.states, "unknown"),
    ypanel="data values"))
viterbi.color.scale <- scale_color_manual(
  "Best state",
  values=state.values)
viterbi.theme <- theme(
  panel.grid.minor=element_blank(),
  legend.key=element_rect(fill="grey"))
viterbi.back <- function(data.hilite){
  some.segs <- back.segs[data.t>=data.hilite+1]
  this.hilite <- some.dt[data.i==data.hilite]
  hilite.data <- data.table(some.dt)
  hilite.data[data.i < data.hilite, best_state_seq := "unknown"]
  gg <- ggplot()+
    scale_x_continuous(
      "Position/index in data sequence",
      breaks=1:nrow(some.dt))+
    scale_y_continuous(
      "")+
    viterbi.color.scale+
    viterbi.theme+
    blank.points+
    geom_point(aes(
      data.i, logratio),
      size=3,
      color="grey",
      data=data.table(some.dt, ypanel="data values"))+
    geom_blank(aes(
      x, y),
      data=data.table(x=c(0, nrow(some.dt)+1), y=1, ypanel))+
    geom_tile(aes(
      data.i, state, fill=log.prob),
      color="transparent",
      data=data.table(max.prob.dt, ypanel))
  if(nrow(this.hilite)){
    gg <- gg+
    geom_tile(aes(
      data.i, as.integer(best_state_seq)),
      color="black",
      size=1,
      fill="transparent",
      data=data.table(this.hilite, ypanel))+
    geom_point(aes(
      data.i, logratio),
      size=3,
      color="black",
      data=data.table(this.hilite, ypanel="data values"))
  }
  if(nrow(some.segs)){
    gg <- gg+
    geom_segment(aes(
      data.t, state.t,
      xend=data.prev, yend=state.prev),
      data=data.table(some.segs, ypanel),
      size=1,
      color="grey")
  }
  gg <- gg+
    geom_point(aes(
      data.i, logratio, color=best_state_seq),
      data=data.table(hilite.data, ypanel="data values"))+
    geom_text(aes(
      data.i, state, label=best_prev),
      size=geom.text.size,
      data=data.table(best.prev.dt, ypanel))+
    scale_fill_gradient(low="white", high="red")+
    facet_grid(ypanel ~ ., scales="free")
  gg
}
viterbi.back(0)

viterbi.forward <- function(data.t, state.k){
  some <- function(DT){
    DT[data.i<data.t | data.i==data.t & state<=state.k]
  }
  hilite.data <- data.table(some.dt)
  hilite.data[, best_state_seq := "unknown"]
  seg.dt <- data.table(
    data.prev=data.t-1,
    data.t,
    state.prev=1:n.states,
    state.k)
  ggplot()+
    theme(panel.grid.minor=element_blank())+
    scale_x_continuous(
      "Position/index in data sequence",
      breaks=1:nrow(some.dt))+
    scale_y_continuous(
      "")+
    viterbi.color.scale+
    viterbi.theme+
    blank.points+
    geom_blank(aes(
      x, y),
      data=data.table(x=c(0, nrow(some.dt)+1), y=1, ypanel))+
    geom_point(aes(
      data.i, logratio),
      size=3,
      color="grey",
      data=data.table(some.dt, ypanel="data values"))+
    geom_point(aes(
      data.i, logratio, color=best_state_seq),
      data=data.table(hilite.data, ypanel="data values"))+
    geom_tile(aes(
      data.i, state, fill=log.prob),
      color="transparent",
      data=data.table(
        some(max.prob.dt),
        ypanel))+
    geom_blank(aes(
      data.i, state, fill=log.prob),
      data=data.table(
        max.prob.dt,
        ypanel))+
    geom_segment(aes(
      data.t, state.k,
      xend=data.prev, yend=state.prev),
      data=data.table(
        seg.dt,
        ypanel),
      size=1,
      color="grey")+
    geom_text(aes(
      data.i, state, label=best_prev),
      size=geom.text.size,
      data=data.table(
        some(best.prev.dt),
        ypanel))+
    scale_fill_gradient(low="white", high="red")+
    facet_grid(ypanel ~ ., scales="free")
}
```

---

# Viterbi forward pass

```{r}
viterbi.forward(2, 1)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(2, 2)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(2, 3)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(3, 1)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(3, 2)
```

---

# Viterbi forward pass

```{r viterbi-forward}
viterbi.forward(3, 3)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(12, 1)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(12, 2)
```

---

# Viterbi forward pass

```{r}
viterbi.forward(12, 3)
```

---

# Viterbi backtracking

```{r}
viterbi.back(12)
```

---

# Viterbi backtracking

```{r}
viterbi.back(11)
```

---

# Viterbi backtracking

```{r}
viterbi.back(10)
```

---

# Viterbi backtracking

```{r}
viterbi.back(9)
```

---

# Viterbi backtracking

```{r}
viterbi.back(8)
```

---

# Viterbi backtracking

```{r}
viterbi.back(7)
```

---

# Viterbi backtracking

```{r}
viterbi.back(6)
```

---

# Viterbi backtracking

```{r}
viterbi.back(5)
```

---

# Viterbi backtracking

```{r}
viterbi.back(4)
```

---

# Viterbi backtracking

```{r viterbi-backtracking}
viterbi.back(3)
```

---

# Viterbi backtracking

```{r}
viterbi.back(2)
```

---

# Viterbi backtracking

```{r}
viterbi.back(1)
```

---

# Baum-Welch learning algorithm

- Is an instance of Expectation-Maximization (EM), like the Gaussian
  Mixture Model learning algorithm.
- E step involves forward/backward passes over data sequences, to
  compute probability of each data point in each state.
- M step involves re-computing model parameters.
- Repeat until the log likelihood stops increasing.

```{r, fig.height=4}
n.states <- 3
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
mean.vec <- c(-1, 0, 1)*0.1
y.vec <- chrom.dt[["logratio"]]
N.data <- length(y.vec)
sd.param <- 1
## Need to compute emission whenever parameters are updated.
log.emission.mat <- dnorm(
  y.vec, 
  matrix(mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec <- log(rep(1/n.states, n.states))
prob.dt.list <- list()
lik.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
viterbi.dt.list <- list()
prior.dt.list <- list()
done <- FALSE
prev.log.lik <- -Inf
iteration <- 1
while(!done){
  fwd.list <- plotHMM::forward_interface(
    log.emission.mat, log.A.mat, log.pi.vec)
  log.alpha.mat <- fwd.list[["log_alpha"]]
  log.beta.mat <- plotHMM::backward_interface(log.emission.mat, log.A.mat)
  log.gamma.mat <- plotHMM::multiply_interface(log.alpha.mat, log.beta.mat)
  prob.mat <- exp(log.gamma.mat)
  log.xi.array <- plotHMM::pairwise_interface(
    log.emission.mat, log.A.mat, log.alpha.mat, log.beta.mat)
  ## update rules.
  (log.pi.vec <- log.gamma.mat[1,])
  (mean.vec <- colSums(y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- y.vec-matrix(mean.vec, N.data, n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  log.A.mat <- plotHMM::transition_interface(
    log.gamma.mat[-N.data,], log.xi.array)
  ## M step done, now store stuff to plot.
  log.emission.mat <- dnorm( #need to compute after M step for viterbi.
    y.vec, 
    matrix(mean.vec, N.data, n.states, byrow=TRUE),
    sd.param,
    log=TRUE)
  viterbi.result <- plotHMM::viterbi_interface(
    log.emission.mat, log.A.mat, log.pi.vec)
  viterbi.dt.list[[iteration]] <- data.table(
    iteration,
    data.i=1:N.data,
    state=factor(viterbi.result[["state_seq"]]))
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(exp(log.A.mat)),
    from.state=as.integer(row(log.A.mat)),
    to.state=as.integer(col(log.A.mat)))
  prior.dt.list[[iteration]] <- data.table(
    iteration,
    state=1:n.states,
    prob=exp(log.pi.vec))
  log.lik <- fwd.list[["log_lik"]]
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik)
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    data.i=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
  iteration <- iteration+1
  if(log.lik <= prev.log.lik){
    done <- TRUE
  }
  prev.log.lik <- log.lik
}
viterbi.dt <- do.call(rbind, viterbi.dt.list)
prob.dt <- do.call(rbind, prob.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
trans.dt <- do.call(rbind, trans.dt.list)
prior.dt <- do.call(rbind, prior.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
expand <- diff(range(y.vec))/10
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)
baum.welch.one <- function(it){
  max.dens <- max(dens.dt$density)
  tile.x.vec <- seq(0, max.dens, l=n.states+2)
  names(tile.x.vec) <- seq(-1, n.states)
  state.y <- seq(1, 0, l=n.states+2)[-c(1, n.states+2)]
  it.viterbi <- viterbi.dt[iteration==it]
  it.mean <- mean.dt[iteration==it]
  it.trans <- trans.dt[iteration==it]
  it.prior <- prior.dt[iteration==it]
  it.lik <- lik.dt[iteration==it]
  it.trans.prior <- rbind(
    it.trans,
    it.prior[, .(iteration, prob, from.state=state, to.state=-1)])
  it.trans.prior[, x := tile.x.vec[paste(to.state)] ]
  it.trans.prior[, y := state.y[from.state] ]
  it.dens <- dens.dt[iteration==it]
  it.prob <- prob.dt[iteration==it]
  state.values <- c("#1B9E77", "#D95F02", "#7570B3")
  chrom.dt[, state := it.viterbi[["state"]] ]
  ggplot()+
    xlab("")+
    ylab("")+
    geom_rect(aes(
      xmin=-Inf, xmax=Inf,
      ymin=mean-sd, ymax=mean+sd),
      size=2,
      alpha=0.4,
      color="transparent",
      fill="grey50",
      data=data.table(ypanel="data values", it.mean))+
    geom_hline(aes(
      yintercept=mean, color=state),
      size=1,
      data=data.table(ypanel="data values", it.mean))+
    geom_text(aes(
      Inf, Inf, label=sprintf("sd=%.4f", sd)),
      size=geom.text.size,
      vjust=1.1,
      hjust=1,
      data=data.table(ypanel="data values", xpanel="density", it.mean[1]))+
    geom_blank(aes(
      density, y),
      data=data.table(xpanel="density", ypanel="data values", dens.dt))+
    geom_tile(aes(
      x, y, fill=prob),
      data=data.table(xpanel="density", ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=sprintf("%.3f", prob)),
      size=geom.text.size,
      data=data.table(xpanel="density", ypanel="probability", it.trans.prior))+
    geom_text(aes(
      0, y, label="emission"),
      size=geom.text.size,
      hjust=0,
      vjust=0,
      data=data.table(
        ypanel="data values", xpanel="density",
        y=max(y.grid)))+
    geom_text(aes(
      x, y, label=state, color=state),
      size=geom.text.size,
      data=data.table(
        xpanel="density", ypanel="probability",
        state=factor(1:n.states),
        x=tile.x.vec[2],
        y=state.y
      ))+
    geom_text(aes(
      x, y, label=type, hjust=hjust),
      size=geom.text.size,
      vjust=1,
      data=data.table(
        xpanel="density", ypanel="probability",
        type=c("initial", "transition"),
        x=tile.x.vec[c(1, n.states+2)],
        hjust=c(0,1),
        y=1
      ))+
    geom_path(aes(
      density, y, color=state),
      data=data.table(xpanel="density", ypanel="data values", it.dens))+
    geom_label(aes(
      0, mean, color=state, label=state),
      hjust=1,
      data=data.table(xpanel="density", ypanel="data values", it.mean))+
    scale_color_manual(values=state.values)+
    scale_fill_gradient(low="white", high="red", limits=c(0,1))+
    geom_point(aes(
      data.i, logratio, color=state),
      data=data.table(xpanel="data.i", ypanel="data values", chrom.dt))+
    geom_text(aes(
      Inf, Inf, label=sprintf(
        "iteration=%d log(lik)=%.4f", it, log.lik)),
      size=geom.text.size,
      hjust=1,
      vjust=1.1,
      data=data.table(xpanel="data.i", ypanel="data values", it.lik))+
    geom_line(aes(
      data.i, prob, color=state),
      data=data.table(xpanel="data.i", ypanel="probability", it.prob))+
    geom_blank(aes(
      data.i, prob),
      data=data.table(
        xpanel="data.i", ypanel="probability", data.i=0, prob=c(0,1)))+
    facet_grid(ypanel ~ xpanel, scales="free")
}
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(1)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(10)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(20)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(30)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(35)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(40)
```

---

# Visualization of learning iterations

```{r}
baum.welch.one(45)
```

---

# Shared states between chromosomes on a profile?

- Sometimes there are several data sequences which are assumed to have
  the same set of hidden states.
- In this case Baum-Welch can be used to fit HMM to all data at the
  same time (total log likelihood is summed over all data sequences).

```{r, fig.height=5}
pro.dt <- nb.dt[profile.id=="4" & chromosome %in% paste(1:10)]
ggplot()+
  facet_grid(. ~ chromosome, scales="free", space="free")+
  scale_x_continuous(
    "Position/index in data sequence",
    breaks=seq(0, 1000, by=100))+
  scale_y_continuous(
    "logratio (data values)")+
  geom_point(aes(
    data.i, logratio),
    data=pro.dt)
```

---

# HMM learned on whole profile

```{r}
pro.dt[, row := 1:.N]
all.y.vec <- pro.dt[["logratio"]]
data.list <- split(pro.dt, paste(pro.dt[["chromosome"]]))
first.row.vec <- pro.dt[data.i==1, row]
last.row.vec <- pro.dt[, .SD[data.i==.N], by=chromosome][["row"]]
n.states <- 4
log.A.mat <- log(matrix(1/n.states, n.states, n.states))
set.seed(1)
mean.vec <- rnorm(n.states)
sd.param <- 1
log.pi.vec <- log(rep(1/n.states, n.states))
prob.dt.list <- list()
lik.dt.list <- list()
prior.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
viterbi.dt.list <- list()
done <- FALSE
prev.log.lik <- -Inf
iteration <- 1
options(warn=2)
while(!done){
  log.lik.vec <- rep(NA, length(data.list))
  all.log.gamma.mat <- matrix(NA, nrow(pro.dt), n.states)
  all.log.xi.array <- array(NA, c(n.states, n.states, nrow(pro.dt)))
  for(chrom.i in seq_along(data.list)){
    one.chrom <- data.list[[chrom.i]]
    row.vec <- one.chrom[["row"]]
    y.vec <- one.chrom[["logratio"]]
    N.data <- length(y.vec)
    log.emission.mat <- dnorm(
      matrix(y.vec, N.data, n.states, byrow=FALSE),
      matrix(mean.vec, N.data, n.states, byrow=TRUE),
      sd.param,
      log=TRUE)
    fwd.list <- plotHMM::forward_interface(
      log.emission.mat, log.A.mat, log.pi.vec)
    log.alpha.mat <- fwd.list[["log_alpha"]]
    log.lik.vec[[chrom.i]] <- fwd.list[["log_lik"]]
    log.beta.mat <- plotHMM::backward_interface(log.emission.mat, log.A.mat)
    all.log.gamma.mat[row.vec, ] <- plotHMM::multiply_interface(
      log.alpha.mat, log.beta.mat)
    all.log.xi.array[,, row.vec[-N.data] ] <- plotHMM::pairwise_interface(
      log.emission.mat, log.A.mat, log.alpha.mat, log.beta.mat)
    viterbi.result <- plotHMM::viterbi_interface(
      log.emission.mat, log.A.mat, log.pi.vec)
    viterbi.dt.list[[paste(iteration, chrom.i)]] <- data.table(
      iteration, chrom.i,
      one.chrom, 
      state=factor(viterbi.result[["state_seq"]]))
  }
  ## update rules.
  (log.pi.vec <- apply(
    all.log.gamma.mat[first.row.vec,]-log(length(first.row.vec)),
    2, plotHMM::logsumexp))
  prob.mat <- exp(all.log.gamma.mat)
  (mean.vec <- colSums(all.y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- all.y.vec-matrix(
    mean.vec, length(all.y.vec), n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  log.A.mat <- plotHMM::transition_interface(
    all.log.gamma.mat[-last.row.vec,],
    all.log.xi.array[,, -last.row.vec])
  prior.dt.list[[iteration]] <- data.table(
    iteration,
    state=1:n.states,
    prob=exp(log.pi.vec))
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(exp(log.A.mat)),
    from.state=as.integer(row(log.A.mat)),
    to.state=as.integer(col(log.A.mat)))
  log.lik <- sum(log.lik.vec)
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik)
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    row=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
  cat(sprintf("iteration=%4d log.lik=%f\n", iteration, log.lik))
  iteration <- iteration+1
  if(log.lik <= prev.log.lik){
    done <- TRUE
  }
  prev.log.lik <- log.lik
}
viterbi.dt <- do.call(rbind, viterbi.dt.list)
prob.dt <- do.call(rbind, prob.dt.list)
prior.dt <- do.call(rbind, prior.dt.list)
trans.dt <- do.call(rbind, trans.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
y.vec <- pro.dt$logratio
expand <- diff(range(y.vec))/10
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
dens.dt[, norm.density := max(pro.dt$data.i)*density/max(density)]
max.dens <- max(dens.dt$norm.density)
ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)
## for debugging https://iulg.sitehost.iu.edu/moss/hmmcalculations.pdf
baum.welch.multiple <- function(it){
  tile.x.vec <- seq(0, max.dens, l=n.states+2)
  names(tile.x.vec) <- seq(-1, n.states)
  state.y <- seq(1, 0, l=n.states+2)[-c(1, n.states+2)]
  it.trans <- trans.dt[iteration==it]
  it.prior <- prior.dt[iteration==it]
  it.dens <- dens.dt[iteration==it]
  it.trans.prior <- rbind(
    it.trans,
    it.prior[, .(iteration, prob, from.state=state, to.state=-1)])
  it.trans.prior[, x := tile.x.vec[paste(to.state)] ]
  it.trans.prior[, y := state.y[from.state] ]
  it.lik <- lik.dt[iteration==it]
  it.mean <- mean.dt[iteration==it]
  it.viterbi <- viterbi.dt[iteration==it]
  it.prob <- prob.dt[iteration==it][pro.dt, on="row"]
  state.values <- c(
    "#E7298A","#7570B3",
    "#1B9E77", "#D95F02")
  cfac <- function(x)factor(x, c(1:10,"density"))
  ggplot()+
    scale_x_continuous(
      "Position/index in data sequence",
      breaks=c(100, 300))+
    ylab("")+
    scale_color_manual(values=state.values)+
    geom_text(aes(
      Inf, Inf, label=sprintf(
        "iteration=%d\nlog(lik)=%.1f", it, log.lik)),
      size=geom.text.size,
      hjust=1,
      vjust=1.1,
      data=data.table(
        it.lik,
        ypanel="data values",
        chromosome=cfac("1")))+
    geom_rect(aes(
      xmin=-Inf, xmax=Inf,
      ymin=mean-sd, ymax=mean+sd),
      size=2,
      alpha=0.4,
      color="transparent",
      fill="grey50",
      data=data.table(ypanel="data values", it.mean))+
    geom_hline(aes(
      yintercept=mean, color=state),
      size=1,
      data=data.table(ypanel="data values", it.mean))+
    geom_point(aes(
      data.i, logratio, color=state),
      data=data.table(ypanel="data values", it.viterbi))+
    geom_path(aes(
      norm.density, y, color=state),
      data=data.table(
        ypanel="data values", chromosome=cfac("density"), it.dens))+
    geom_blank(aes(
      norm.density, y),
      data=data.table(
        ypanel="data values", chromosome=cfac("density"), dens.dt))+
    geom_label(aes(
      0, mean, color=state, label=state),
      hjust=1,
      data=data.table(
        chromosome=cfac("density"), ypanel="data values", it.mean))+
    geom_tile(aes(
      x, y, fill=prob),
      data=data.table(
        chromosome=cfac("density"), ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=sprintf("%.2f", prob)),
      angle=90,
      size=geom.text.size,
      data=data.table(
        chromosome=cfac("density"), ypanel="probability", it.trans.prior))+
    geom_text(aes(
      x, y, label=state, color=state),
      size=geom.text.size,
      data=data.table(
        chromosome=cfac("density"), ypanel="probability",
        state=factor(1:n.states),
        x=tile.x.vec[2],
        y=state.y
      ))+
    geom_text(aes(
      x, y, label=type, hjust=hjust),
      vjust=1.1,
      size=geom.text.size,
      data=data.table(
        chromosome=cfac("density"), ypanel="probability",
        type=c("initial", "transition"),
        x=c(-Inf, Inf),
        hjust=c(0,1),
        y=Inf
      ))+
    geom_text(aes(
      Inf, Inf, label=sprintf("emission\nsd=%.4f", sd)),
      vjust=1.1,
      hjust=1,
      size=geom.text.size,
      data=data.table(
        ypanel="data values", chromosome=cfac("density"), it.mean[1]))+
    scale_fill_gradient(low="white", high="red", limits=c(0,1))+
    geom_line(aes(
      data.i, prob, color=state),
      data=data.table(ypanel="probability", it.prob))+
    geom_blank(aes(
      data.i, prob),
      data=data.table(ypanel="probability", data.i=0, prob=c(0,1)))+
    facet_grid(ypanel ~ chromosome, scales="free", space="free_x")
}
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(1)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(5)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(10)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(15)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(20)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(25)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(30)
```

---

# Learning iterations using multiple sequences

```{r}
baum.welch.multiple(35)
```

---

# Time/space complexity 

For $K$ states and $N$ data

- Forward/Viterbi: $O(K^2 N)$ time, $O(K^2 + K N)$ space.
- Each iteration of Baum-Welch: $O(K^2 N)$ time and space.
- Some asymptotic speedups are possible in special cases, for example
  sparse transition matrices, see Murphy book.

---

# Comparison with other algorithms

- K-means and Gaussian mixture models also have cluster-specific
  parameters (mean, covariance, prior weight), but are not able to
  model sequential dependence.
- All segmentation models we studied had $K$ mean parameters, and a
  single variance/sd parameter common to all segments.
- Binary/optimal segmentation require specification of number of
  segments/changepoints (always jump to a new mean parameter) whereas
  HMM requires number of hidden states (may jump to a previously
  visited mean parameter).
- Binary/optimal segmentation with $K$ segments can be interpreted as
  an HMM with a constrained transition matrix ($a_{ij}=1$ if $j=i+1$
  else 0: always jump to the next state, never jump back to a previous
  state).
- Binary/optimal segmentation log likelihood only uses emission
  probabilities, whereas HMM also includes initial/transition
  probabilities.

---

# Possible exam questions

- In the previous slides we saw $K=3$ or 4 clusters. How many
  parameters of each type are there to learn in each case? (assume
  common sd parameter as in slides)
- How many parameters if each cluster has its own sd parameter?
- In the previous slides the data have a single feature
  (logratio). How many parameters if there are $P=2$ real-valued
  features instead? (assume normal distribution with no constraints on
  covariance matrix)
