---
title: "Hidden Markov Models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
setkey(nb.dt, profile.id, chromosome)
one.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position, logratio),
    data=one.dt)
```

---

# Motivation for Hidden Markov Models (HMMs)

- Sometimes we have an interpretation / expectation of what the
  segments/clusters mean.
- For example in DNA copy number data the logratio=0 means normal copy
  number (two copies -- one from each parent), whereas higher logratio
  values indicate gain/amplification and lower values indicate
  loss/deletion.
- HMMs have a hidden state variable which is associated with each
  observed data variable in the sequence.
- There are typically a finite number of hidden states/clusters,
  $k\in\{1,\dots,K\}$.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some emission distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.
- These parameters are unknown in advance and must be learned from the
  data.
- Comparison with Gaussian Mixture Models: HMM has all of GMM
  parameters, plus transition matrix.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.

---

# Computations

```{r}
n.states <- 3
A.mat <- matrix(1/n.states, n.states, n.states)
mean.vec <- c(-1, 0, 1)*0.1
y.vec <- one.dt[["logratio"]]
sd.param <- 1
pi.vec <- rep(1/n.states, n.states)
N.data <- length(y.vec)
prob.dt.list <- list()
lik.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
max.it <- 50
for(iteration in 1:max.it){
  log.emission.mat <- dnorm(
    matrix(y.vec, N.data, n.states, byrow=FALSE),
    matrix(mean.vec, N.data, n.states, byrow=TRUE),
    sd.param,
    log=TRUE)
  fwd.list <- plotHMM::forward_interface(log.emission.mat, A.mat, pi.vec)
  log.alpha.mat <- fwd.list[["log_alpha"]]
  log.beta.mat <- plotHMM::backward_interface(log.emission.mat, A.mat)
  log.gamma.mat <- plotHMM::multiply_interface(log.alpha.mat, log.beta.mat)
  prob.mat <- exp(log.gamma.mat)
  log.xi.array <- plotHMM::pairwise_interface(
    log.emission.mat, A.mat, log.alpha.mat, log.beta.mat)
  ## update rules.
  (pi.vec <- prob.mat[1,])
  (mean.vec <- colSums(y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- y.vec-matrix(mean.vec, N.data, n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  A.mat <- plotHMM::transition_interface(log.gamma.mat, log.xi.array)
  ##apply(log.gamma.mat, 1, which.max)
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(A.mat),
    from.state=as.integer(row(A.mat)),
    to.state=as.integer(col(A.mat)))
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik=fwd.list[["log_lik"]])
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    data.i=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
}
prob.dt <- do.call(rbind, prob.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
expand <- diff(range(y.vec))/10
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
## https://web.stanford.edu/~jurafsky/slp3/A.pdf

ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)


## comparing sd with binseg.
segs <- coef(binsegRcpp::binseg_normal(y.vec, 4), 4L)
setkey(segs, start, end)
one.dt[, i := .I]
one.dt[, data.i := .I]
setkey(one.dt, i, data.i)
over.dt <- foverlaps(one.dt, segs)
over.dt[, resid := logratio-mean]
over.dt[, sqrt(mean(resid^2))]
## TODO draw transition matrix.
it <- 41
it.mean <- mean.dt[iteration==it]
it.dens <- dens.dt[iteration==it]
it.prob <- prob.dt[iteration==it]
dput(RColorBrewer::brewer.pal(3, "Dark2"))
state.values <- c("#1B9E77", "#D95F02", "#7570B3")
gg <- ggplot()+
  xlab("")+
  ylab("")+
  geom_text(aes(
    0, mean, color=state, label=paste(state, " ")),
    hjust=1,
    data=data.table(xpanel="dens", ypanel="data values", it.mean))+
  geom_blank(aes(
    density, y),
    data=data.table(xpanel="dens", ypanel="data values", dens.dt))+
  geom_path(aes(
    density, y, color=state),
    data=data.table(xpanel="dens", ypanel="data values", it.dens))+
  scale_color_manual(values=state.values)+
  geom_point(aes(
    data.i, logratio),
    data=data.table(xpanel="data.i", ypanel="data values", one.dt))+
  geom_blank(aes(
    data.i, prob),
    data=data.table(xpanel="data.i", ypanel="prob", prob.dt))+
  geom_line(aes(
    data.i, prob, color=state),
    data=data.table(xpanel="data.i", ypanel="prob", it.prob))+
  facet_grid(ypanel ~ xpanel, scales="free")
gg

```

