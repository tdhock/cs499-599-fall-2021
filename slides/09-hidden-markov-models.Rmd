---
title: "Hidden Markov Models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
data(neuroblastoma, package="neuroblastoma")
nb.dt <- data.table(neuroblastoma[["profiles"]])
setkey(nb.dt, profile.id, chromosome)
one.dt <- nb.dt[J("4", "2")]
ggplot()+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position, logratio),
    data=one.dt)
```

---

# Motivation for Hidden Markov Models (HMMs)

- Sometimes we have an interpretation / expectation of what the
  segments/clusters mean.
- For example in DNA copy number data the logratio=0 means normal copy
  number (two copies -- one from each parent), whereas higher logratio
  values indicate gain/amplification and lower values indicate
  loss/deletion.
- HMMs have a hidden state variable which is associated with each
  observed data variable in the sequence.
- There are typically a finite number of hidden states/clusters,
  $k\in\{1,\dots,K\}$.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some emission distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.
- These parameters are unknown in advance and must be learned from the
  data.
- Comparison with Gaussian Mixture Models: HMM has all of GMM
  parameters, plus transition matrix.

---

# Baum-Welch learning algorithm

Let $K$ be the number of hidden states. For every state we have
- Let $A\in\mathbb [0,1]^{K\times K}$ be the transition matrix. Each
  entry $a_{ij}$ is the probability of of being in state $i$ at some
  time point $t$ and then at state $j$ at the next time point $t+1$.
- Let $\pi_k$ be the initial state distribution (prior weights). 
- There are also some distribution-specific parameters (mean,
  covariance) that are used to define the probablity of observing data
  $y$ in state $k$, $b_j(y)\in[0,1]$.

---

# Computations

```{r}
n.states <- 3
A.mat <- matrix(1/n.states, n.states, n.states)
mean.vec <- c(-1, 0, 1)*0.1
y.vec <- one.dt[["logratio"]]
n.q <- n.states+2
mean.vec <- quantile(y.vec, seq(0, 1, l=n.q)[-c(1,n.q)])
##set.seed(1)
##mean.vec <- rnorm(n.states)
sd.param <- 1
pi.vec <- rep(1/n.states, n.states)
N.data <- length(y.vec)
##http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf
elnsum <- function(elnx, elny){
  fcase(
    elnx == -Inf, elny,
    elny == -Inf, elnx,
    elny < elnx, elnx + log(1+exp(elny-elnx)),
    elnx <= elny, elny + log(1+exp(elnx-elny)))
}
elnproduct <- function(elnx, elny){
  ifelse(
    elnx == -Inf | elny == -Inf,
    -Inf,
    elnx+elny)
}

prob.dt.list <- list()
lik.dt.list <- list()
trans.dt.list <- list()
mean.dt.list <- list()
max.it <- 50
for(iteration in 1:max.it){
  log.emission.mat <- dnorm(
    matrix(y.vec, N.data, n.states, byrow=FALSE),
    matrix(mean.vec, N.data, n.states, byrow=TRUE),
    sd.param,
    log=TRUE)
  ##alpha/forward.
  log.alpha.mat <- matrix(NA, N.data, n.states)
  log.alpha.mat[1,] <- elnproduct(
    log(pi.vec),
    log.emission.mat[1,])
  for(data.t in 2:N.data){
    log.alpha.vec <- rep(-Inf, n.states)
    for(state.i in 1:n.states){
      prod.vec <- elnproduct(
        log.alpha.mat[data.t-1,state.i],
        log(A.mat[state.i,]))
      log.alpha.vec <- elnsum(log.alpha.vec, prod.vec)
    }
    emission.prob.vec <- log.emission.mat[data.t,]
    log.alpha.mat[data.t,] <- elnproduct(log.alpha.vec, emission.prob.vec)
  }
  logsumexp <- function(exponents.vec){
    m <- max(exponents.vec)
    m + log(sum(exp(exponents.vec-m)))
  }
  print(log.lik <- logsumexp(log.alpha.mat[N.data,]))
  ##beta/backward.
  log.beta.mat <- matrix(NA, N.data, n.states)
  log.beta.mat[N.data, ] <- 0
  for(data.t in seq(N.data-1, 1)){
    for(state.i in 1:n.states){
      log.beta <- -Inf
      for(state.j in 1:n.states){
        le <- log.emission.mat[data.t+1,state.j]
        lb <- log.beta.mat[data.t+1,state.j]
        la <- log(A.mat[state.i,state.j])
        inside.product <- elnproduct(le, lb)
        outside.product <- elnproduct(la, inside.product)
        log.beta <- elnsum(log.beta, outside.product)
      }
      log.beta.mat[data.t,state.i] <- log.beta
    }
  }
  log.gamma.mat <- matrix(NA, N.data, n.states)
  normalizer <- rep(-Inf, N.data)
  for(state.i in 1:n.states){
    log.gamma.mat[,state.i] <- elnproduct(
      log.alpha.mat[,state.i],log.beta.mat[,state.i])
    normalizer <- elnsum(normalizer, log.gamma.mat[,state.i])
  }
  for(state.i in 1:n.states){
    log.gamma.mat[,state.i] <- elnproduct(
      log.gamma.mat[,state.i], -normalizer)
  }
  prob.mat <- exp(log.gamma.mat)
  xi.arr <- array(NA, c(n.states, n.states, N.data-1))
  for(data.t in seq(1, N.data-1)){
    normalizer <- -Inf
    for(state.i in 1:n.states){
      for(state.j in 1:n.states){
        first.product <- elnproduct(
          log.emission.mat[data.t+1, state.j],
          log.beta.mat[data.t+1, state.j])
        second.product <- elnproduct(
          first.product,
          log(A.mat[state.i, state.j]))
        value <- elnproduct(
          second.product,
          log.alpha.mat[data.t, state.i])
        normalizer <- elnsum(normalizer, value)
        xi.arr[state.i, state.j, data.t] <- value
      }
    }
    xi.arr[,, data.t] <- elnproduct(xi.arr[,, data.t], -normalizer)
  }
  ## update rules.
  (pi.vec <- prob.mat[1,])
  (mean.vec <- colSums(y.vec*prob.mat)/colSums(prob.mat))
  resid.mat <- y.vec-matrix(mean.vec, N.data, n.states, byrow=TRUE)
  var.est <- sum(prob.mat * resid.mat^2) / sum(prob.mat)
  (sd.param <- sqrt(var.est))
  ##apply(log.gamma.mat, 1, which.max)
  for(state.i in 1:n.states){
    for(state.j in 1:n.states){
      numerator <- -Inf
      denominator <- -Inf
      for(data.t in seq(1, N.data-1)){
        numerator <- elnsum(numerator, xi.arr[state.i, state.j, data.t])
        denominator <- elnsum(denominator, log.gamma.mat[data.t, state.i])
      }
      A.mat[state.i, state.j] <- exp(elnproduct(numerator, -denominator))
    }
  }
  trans.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(A.mat),
    from.state=as.integer(row(A.mat)),
    to.state=as.integer(col(A.mat)))
  lik.dt.list[[iteration]] <- data.table(
    iteration,
    log.lik)
  mean.dt.list[[iteration]] <- data.table(
    iteration,
    state=factor(seq_along(mean.vec)),
    sd=sd.param,
    mean=mean.vec)
  prob.dt.list[[iteration]] <- data.table(
    iteration,
    prob=as.numeric(prob.mat),
    data.i=as.integer(row(prob.mat)),
    state=factor(as.integer(col(prob.mat))))
}
prob.dt <- do.call(rbind, prob.dt.list)
mean.dt <- do.call(rbind, mean.dt.list)
lik.dt <- do.call(rbind, lik.dt.list)
y.grid <- sort(c(
  mean.dt$mean,
  seq(min(y.vec)-expand, max(y.vec)+expand, l=201)))
dens.dt <- mean.dt[, .(
  y=y.grid,
  density=dnorm(y.grid, mean, sd)
), by=.(iteration, state)]
## https://web.stanford.edu/~jurafsky/slp3/A.pdf

ggplot()+
  geom_point(aes(
    iteration, log.lik),
    data=lik.dt)


## TODO draw transition matrix.
it <- 1
it.mean <- mean.dt[iteration==it]
it.dens <- dens.dt[iteration==it]
expand <- diff(range(y.vec))/10
it.prob <- prob.dt[iteration==it]
dput(RColorBrewer::brewer.pal(3, "Dark2"))
state.values <- c("#1B9E77", "#D95F02", "#7570B3")
gg <- ggplot()+
  xlab("")+
  ylab("")+
  geom_text(aes(
    0, mean, color=state, label=paste(state, " ")),
    hjust=1,
    data=data.table(xpanel="dens", ypanel="data values", it.mean))+
  geom_blank(aes(
    density, y),
    data=data.table(xpanel="dens", ypanel="data values", dens.dt))+
  geom_path(aes(
    density, y, color=state),
    data=data.table(xpanel="dens", ypanel="data values", it.dens))+
  scale_color_manual(values=state.values)+
  geom_point(aes(
    data.i, logratio),
    data=data.table(xpanel="data.i", ypanel="data values", one.dt))+
  geom_blank(aes(
    data.i, prob),
    data=data.table(xpanel="data.i", ypanel="prob", prob.dt))+
  geom_line(aes(
    data.i, prob, color=state),
    data=data.table(xpanel="data.i", ypanel="prob", it.prob))+
  facet_grid(ypanel ~ xpanel, scales="free")
gg

## comparing sd with binseg.
segs <- coef(binsegRcpp::binseg_normal(y.vec, 4), 4L)
setkey(segs, start, end)
one.dt[, i := .I]
one.dt[, data.i := .I]
setkey(one.dt, i, data.i)
over.dt <- foverlaps(one.dt, segs)
over.dt[, resid := logratio-mean]
over.dt[, sqrt(mean(resid^2))]
```

```{r}
## for debugging https://iulg.sitehost.iu.edu/moss/hmmcalculations.pdf
st <- function(...)structure(c(...), names=c("s","t"))
pi.vec <- st(.85, .15)
initial.dist <- rbind(
  A=st(.4, .5),
  B=st(.6, .5))
A.mat <- rbind(
  s=st(.3, .7),
  t=st(.1, .9))
seq1=strsplit("ABBA",split="")[[1]]
N.data <- length(seq1)
log.emission.mat <- log(initial.dist[seq1,])
n.states <- length(pi.vec)
log.alpha.mat <- matrix(NA, N.data, n.states)
log.alpha.mat[1,] <- elnproduct(
  log(pi.vec),
  log.emission.mat[1,])
(log.alpha.mat[1,] <- log(c(.34, .08)))
for(data.t in 2:N.data){
  log.alpha.vec <- rep(-Inf, n.states)
  for(state.i in 1:n.states){
    prod.vec <- elnproduct(
      log.alpha.mat[data.t-1,state.i],
      log(A.mat[state.i,]))
    log.alpha.vec <- elnsum(log.alpha.vec, prod.vec)
  }
  emission.prob.vec <- log.emission.mat[data.t,]
  log.alpha.mat[data.t,] <- elnproduct(log.alpha.vec, emission.prob.vec)
}
log.beta.mat <- matrix(NA, N.data, n.states)
log.beta.mat[N.data, ] <- 0
for(data.t in seq(N.data-1, 1)){
  for(state.i in 1:n.states){
    log.beta <- -Inf
    for(state.j in 1:n.states){
      le <- log.emission.mat[data.t+1,state.j]
      lb <- log.beta.mat[data.t+1,state.j]
      la <- log(A.mat[state.i,state.j])
      print(exp(c(la, le, lb)))
      inside.product <- elnproduct(le, lb)
      outside.product <- elnproduct(la, inside.product)
      log.beta <- elnsum(log.beta, outside.product)
    }
    log.beta.mat[data.t,state.i] <- log.beta
  }
}

log.gamma.mat <- matrix(NA, N.data, n.states)
normalizer <- rep(-Inf, N.data)
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.alpha.mat[,state.i],log.beta.mat[,state.i])
  normalizer <- elnsum(normalizer, log.gamma.mat[,state.i])
}
for(state.i in 1:n.states){
  log.gamma.mat[,state.i] <- elnproduct(
    log.gamma.mat[,state.i], -normalizer)
}
prob.mat <- exp(log.gamma.mat)

xi.arr <- array(NA, c(n.states, n.states, N.data-1))
for(data.t in seq(1, N.data-1)){
  normalizer <- -Inf
  for(state.i in 1:n.states){
    for(state.j in 1:n.states){
      first.product <- elnproduct(
        log.emission.mat[data.t+1, state.j],
        log.beta.mat[data.t+1, state.j])
      second.product <- elnproduct(
        first.product,
        log(A.mat[state.i, state.j]))
      value <- elnproduct(
        second.product,
        log.alpha.mat[data.t, state.i])
      normalizer <- elnsum(normalizer, value)
      xi.arr[state.i, state.j, data.t] <- value
    }
  }
  xi.arr[,, data.t] <- elnproduct(xi.arr[,, data.t], -normalizer)
}


```


```{r}
alpha.mat[1,] <- pi.vec * exp(log.emission.mat[1,])
for(data.t in 2:N.data){
  alpha.mat[data.t,] <- exp(log.emission.mat[data.t,])*
    colSums(A.mat*matrix(alpha.mat[data.t-1,], n.states, n.states))
}
mlapp.alpha.mat <- matrix(NA, N.data, n.states)
prob.vec <- pi.vec * exp(log.emission.mat[1,])
Z <- sum(prob.vec)
mlapp.alpha.mat[1,] <- prob.vec/Z
Z.vec <- rep(NA, N.data)
Z.vec[1] <- Z
for(data.t in 2:N.data){
  emission.vec <- exp(log.emission.mat[data.t,])
  prob.vec <- emission.vec * t(A.mat) %*% mlapp.alpha.mat[data.t-1,]
  Z <- sum(prob.vec)
  mlapp.alpha.mat[data.t,] <- prob.vec/Z
  Z.vec[data.t] <- Z
}
## MLAPP equation (17.49) log(x_{1:T}|theta) = sum_{t=1}^T log(Z_t)
logsumexp <- function(exponents.vec){
  m <- max(exponents.vec)
  m + log(sum(exp(exponents.vec-m)))
}
sum(log(Z.vec))
```
