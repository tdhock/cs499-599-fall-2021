---
title: "Segmentation model selection and evaluation"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=7)
```

# Background: detecting abrupt changes is important 

Example from cancer diagnosis: breakpoints are associated with
aggressive disease in neuroblastoma.

```{r}
suppressPackageStartupMessages({
  library(data.table)
  library(ggplot2)
})
set.colors <- c(subtrain="black", validation="red")
theme_set(
  theme_bw()+
    theme(
      panel.spacing=grid::unit(0, "lines"),
      text=element_text(size=20)))
geom.text.size <- 5
data(neuroblastoma, package="neuroblastoma")
nb.ann <- data.table(neuroblastoma$annotations)
set.pid.chr <- function(DT){
  DT[, pid.chr := paste0(profile.id, ".", chromosome)]
  setkey(DT, pid.chr)
}
set.pid.chr(nb.ann)
setkey(nb.ann, pid.chr)
pro.ann <- nb.ann[profile.id=="11"]
nb.dt <- data.table(neuroblastoma[["profiles"]])
nb.dt[, data.i := rank(position), keyby=.(profile.id, chromosome)]
set.pid.chr(nb.dt)

max.segments <- 10
data.dt.list <- list()
model.dt.list <- list()
cv.dt.list <- list()
penalty.dt.list <- list()
selected.dt.list <- list()
model.segs.list <- list()
label.error.list <- list()
subtrain.segs.list <- list()
some.ids <- c(
  "4.2", "4.11",
  "2.3", "2.4", "2.11",
  pro.ann$pid.chr)
for(id in some.ids){
  chrom.dt <- nb.dt[id]
  sub.valid.runs <- rep(c("subtrain", "validation"), each=12)
  chrom.dt[, set := rep(sub.valid.runs, l=.N)]
  set.seed(4)
  chrom.dt[, set := sample(rep(c("subtrain", "validation"), l=.N))]
  binseg.model <- binsegRcpp::binseg_normal(
    chrom.dt[["logratio"]], max.segments)
  loss <- binseg.model[["loss"]]
  n.data <- nrow(chrom.dt)
  penalty.vec <- c(AIC=2, BIC=log(n.data))
  chrom.penalty.list <- list()
  for(penalty.name in names(penalty.vec)){
    penalty.value <- penalty.vec[[penalty.name]]
    model.size <- 1:max.segments
    cost <- loss + penalty.value * model.size
    chrom.penalty.list[[penalty.name]] <- data.table(
      penalty.name,
      segments=model.size,
      cost)
  }
  chrom.penalty <- do.call(rbind, chrom.penalty.list)
  penalty.dt.list[[id]] <- chrom.penalty
  first.pos <- chrom.dt[1, position]
  last.pos <- chrom.dt[.N, position]
  pos.between <- chrom.dt[, position[-1]-diff(position)/2]
  chrom.dt[, start.pos := c(first.pos, pos.between)]
  chrom.dt[, end.pos := c(pos.between, last.pos)]
  data.dt.list[[id]] <- chrom.dt
  subtrain.data <- chrom.dt[set=="subtrain"]
  subtrain.data[, change.before := as.integer(
    position-c(NA, diff(position)/2))]
  subtrain.model <- binsegRcpp::binseg_normal(
    subtrain.data[["logratio"]], max.segments)
  subtrain.segs <- coef(subtrain.model)
  segs.pos <- subtrain.segs[, {
    change.i <- start[start>1]
    change.pos <- subtrain.data$change.before[change.i]
    data.table(
      mean,
      start.pos=c(first.pos, change.pos+1),
      end.pos=c(change.pos, last.pos))
  }, by=segments]
  subtrain.segs.list[[id]] <- segs.pos
  join.dt <- chrom.dt[
    segs.pos,
    .(segments, logratio, mean, set),
    on=.(position >= start.pos, position <= end.pos)]
  chrom.cv <- join.dt[, .(
    n.data=.N,
    error=sum((logratio-mean)^2)
  ), by=.(segments, set)]
  chrom.selected <- rbind(
    chrom.cv[set=="validation", .(algorithm="CV", segments, cost=error)],
    chrom.penalty[, .(algorithm=penalty.name, segments, cost)]
  )[, .SD[which.min(cost)], by=algorithm]
  max.show <- 10
  bad <- chrom.selected[segments>max.show]
  if(nrow(bad)){
    print(bad)
    stop("selected size greater than max")
  }
  selected.dt.list[[id]] <- chrom.selected
  cv.dt.list[[id]] <- chrom.cv
  show.models <- binseg.model[1:max.show]
  chrom.segs <- coef(show.models)
  for(start.or.end in c("start", "end")){
    start.or.end.pos <- paste0(start.or.end, ".pos")
    indices <- chrom.segs[[start.or.end]]
    set(
      chrom.segs,
      j=start.or.end.pos,
      value=chrom.dt[[start.or.end.pos]][indices])
  }
  chrom.ann <- nb.ann[id]
  change.dt <- chrom.segs[start>1]
  change.dt[, pid.chr := id]
  show.models[, pid.chr := id]
  err.list <- penaltyLearning::labelError(
    show.models,
    chrom.ann,
    change.dt,
    change.var="start.pos",
    problem.vars="pid.chr",
    model.vars="segments")
  label.error.list[[id]] <- err.list$label.errors
  model.segs.list[[id]] <- chrom.segs
  model.dt.list[[id]] <- binseg.model
}
plotData <- function(id){
  ggplot()+
    scale_x_continuous(
      "Position/index in data sequence")+
    scale_y_continuous(
      "logratio (approximate DNA copy number)")+
    geom_point(aes(
      data.i, logratio),
      data=data.dt.list[[id]])
}
plotData("4.2")
```

---

# Motivation for segmentation model selection and evaluation

- In each of the segmentation models we have studied, there is a
  choice of model size (segments/changepoints or hidden states).
- Too large model sizes result in false positives (changepoints
  predicted by algorithm but they are not significant/real).
- Too small model sizes result in false negatives (no changepoint
  predicted where there should be).
- Want to maximize true positive rate (number of correctly predicted
  changepoints) and true negative rate (number of correct predicted
  regions without changepoints).

---

# Model selection via Classic Information Criteria 

For every model size $k\in\{1,\dots,K\}$ let $L_k$ be the loss.

Information criteria choose the model which minimizes the penalized
cost, for some non-negative penalty $\lambda\geq 0$,

$$ C(\lambda) = \min_{
k\in\{1,\dots,K\} 
} L_k + \lambda k $$

- BIC=Bayesian Information Criterion, sometimes referred to as
  SIC=Schwarz who was the author. $\lambda=\log n$ where $n$ is the
  number of data points.
- AIC=Akaike Information Criterion: $\lambda=2$.
- Data viz: http://bl.ocks.org/tdhock/raw/43ac9c6be9188dcb02a7/

---

# Model selection criteria plot for binary segmentation

```{r}
pen.colors <- c(BIC="orange", AIC="blue")
pen.color.scale <- scale_color_manual(values=pen.colors)
plotPenalties <- function(id){
  penalty.dt <- penalty.dt.list[[id]]
  gg <- ggplot()+
    pen.color.scale+
    geom_line(aes(
      segments, cost, color=penalty.name),
      data=penalty.dt)+
    scale_x_continuous(
      limits=c(1, max.segments+1),
      breaks=seq(1, max.segments))
  directlabels::direct.label(gg, list(cex=2, "right.polygons"))
}
plotPenalties("4.2")
```

---

# Cross-validation for model selection

```{r}
plotDataCV <- function(id){
  ggplot()+
    scale_color_manual(values=set.colors)+
    scale_x_continuous(
      "Position/index in data sequence")+
    scale_y_continuous(
      "logratio (approximate DNA copy number)")+
    geom_point(aes(
      data.i, logratio, color=set),
      data=data.dt.list[[id]])
}
plotDataCV("4.2")
```

---

# Idea for cross-validation

- Divide full data sequence into subtrain and validation sets.
- Use subtrain data as input to learning algorithm.
- Compute predicted changepoint positions and segment parameters using
  only subtrain data.
- Assign parameters to validation data based on predicted
  changepoints.
- Use validation data to choose best model size (min error or negative
  log likelihood).
- As model size increases, subtrain error should always decrease,
  whereas validation error should be U shaped.

---

# Fitting model to validation set

```{r}
id <- "4.2"
point.dt <- data.dt.list[[id]]
segment.dt <- subtrain.segs.list[[id]]
text.dt <- cv.dt.list[[id]]
model.color <- "deepskyblue"
ggplot()+
  scale_color_manual(values=set.colors)+
  scale_x_continuous(
    "Position/index in data sequence")+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")+
  geom_point(aes(
    position/1e6, logratio, color=set),
    data=point.dt)+
  geom_segment(aes(
    start.pos/1e6, mean,
    xend=end.pos/1e6, yend=mean),
    color=model.color,
    size=1,
    data=segment.dt)+
  geom_vline(aes(
    xintercept=start.pos/1e6),
    color=model.color,
    data=segment.dt[start.pos>min(start.pos)])+
  facet_grid(segments ~ .)+
  geom_text(aes(
    ifelse(set=="subtrain", -Inf, Inf),
    -Inf,
    color=set,
    hjust=ifelse(set=="subtrain", 0, 1),
    label=sprintf(
      "%s error=%.2f",
      set, error)),
    vjust=-0.5,
    data=text.dt)
```

---

# CV Error plot

```{r}
plotErrorCV <- function(id){
  ggplot()+
    scale_color_manual(values=set.colors)+
    geom_line(aes(
      segments, error, color=set),
      data=cv.dt.list[[id]])+
    scale_x_continuous(breaks=seq(1, max.segments))
}
plotErrorCV("4.2")
```

---

# Another data set

```{r}
plotData("4.11")
```

---

# Model selection plot

```{r}
plotPenalties("4.11")
```

---

# Cross-validation for model selection

```{r}
plotDataCV("4.11")
```

---

# CV error plot

```{r}
plotErrorCV("4.11")
```

---

# Labeled regions for evaluating accuracy of changepoint predictions

- After we have selected one penalty/model, how to quantify its error
  rate? (false positive, false negative)
- In general with real data, this is a difficult/unsolved problem.
- Sometimes labels can be created (prior knowledge, visual inspection).
- Like in unsupervised clustering the labels are assumed to be only
  available after the model has been learned. 

---

# Labeled regions for evaluating accuracy of changepoint predictions

```{r}
plotDataLabel <- function(id){
  pro.ann <- nb.ann[id]
  ggplot()+
    scale_x_continuous(
      "Position on chromosome (bases)")+
    scale_y_continuous(
      "logratio (approximate DNA copy number)")+
    geom_rect(aes(
      xmin=min, xmax=max,
      ymin=-Inf, ymax=Inf,
      fill=annotation),
      alpha=0.5,
      data=pro.ann)+
    geom_point(aes(
      position, logratio),
      shape=1,
      data=data.dt.list[[id]])+
    scale_fill_manual(
      values=c(breakpoint="violet", normal="orange"))
}
plotDataLabel("4.11")
```

---

# False negative for missing change in positive label

```{r}
plotDataLabelModels <- function(id){
  seg.dt <- model.segs.list[[id]]
  change.dt <- seg.dt[start>1]
  data.dt <- data.dt.list[[id]]
  ggplot()+
    facet_grid(segments ~ .)+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      color=model.color,
      size=2,
      data=seg.dt)+
    geom_vline(aes(
      xintercept=start.pos),
      data=change.dt,
      size=1,
      color=model.color)+
    scale_x_continuous(
      "Position on chromosome (bases)")+
    scale_y_continuous(
      "logratio (approximate DNA copy number)")+
    geom_point(aes(
      position, logratio),
      shape=1,
      data=data.dt)+
    geom_rect(aes(
      xmin=min, xmax=max,
      ymin=-Inf, ymax=Inf,
      fill=annotation),
      alpha=0.5,
      data=nb.ann[id])+
    geom_rect(aes(
      xmin=min, xmax=max,
      ymin=-Inf, ymax=Inf,
      linetype=status),
      color="black",
      size=2,
      fill="transparent",
      data=label.error.list[[id]])+
    scale_linetype_manual(
      "error type",
      values=c(
        correct=0,
        "false negative"=3,
        "false positive"=1))+
    scale_fill_manual(
      values=c(breakpoint="violet", normal="orange"))
}
plotDataLabelModels("4.11")
```

---

# False positive for changepoint in negative label

```{r}
plotDataLabelModels("2.3")
```

# Comparing model selection algorithms

```{r}
data(neuroblastomaProcessed, package="penaltyLearning")
pen.list <- with(neuroblastomaProcessed, list(
  BIC=feature.mat[, "log.n"],
  AIC=rep(2, nrow(feature.mat))))
pen.dt.list <- list()
for(penalty.name in names(pen.list)){
  pen.dt.list[[penalty.name]] <- data.table(
    penalty.name,
    pid.chr=rownames(neuroblastomaProcessed$feature.mat),
    penalty.value=pen.list[[penalty.name]])
}
pen.dt <- do.call(rbind, pen.dt.list)
pen.dt[, pred.log.lambda := log(penalty.value)]
nb.err <- data.table(neuroblastomaProcessed$errors)
set.pid.chr(nb.err)
selected.errors <- nb.err[pen.dt, on=.(
  pid.chr, min.lambda <= penalty.value, max.lambda >=penalty.value)]
selected.tall <- melt(
  selected.errors,
  measure.vars=c("fp","fn"))
total.tall <- selected.tall[, .(
  total=sum(value)
), by=.(profile.id, penalty.name, variable)]
total.wide <- dcast(
  total.tall,
  profile.id + variable ~ penalty.name,
  value.var="total")
total.wide[, diff := AIC-BIC]
diff.wide <- dcast(
  total.wide,
  profile.id ~ variable,
  value.var="diff")
diff.wide[fp != 0 & fn != 0]

comparePenalties <- function(id){
  selected.dt <- selected.dt.list[[id]]
  chrom.segs <- model.segs.list[[id]]
  chrom.err <- label.error.list[[id]]
  selected.err <- chrom.err[selected.dt, on="segments"]
  selected.segs <- chrom.segs[selected.dt, on="segments"]
  ggplot()+
    facet_grid(algorithm ~ .)+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      color=model.color,
      size=2,
      data=selected.segs)+
    geom_vline(aes(
      xintercept=start.pos),
      data=selected.segs[start>1],
      size=1,
      color=model.color)+
    scale_x_continuous(
      "Position on chromosome (bases)")+
    scale_y_continuous(
      "logratio (approximate DNA copy number)")+
    geom_point(aes(
      position, logratio),
      shape=1,
      data=data.dt.list[[id]])+
    geom_rect(aes(
      xmin=min, xmax=max,
      ymin=-Inf, ymax=Inf,
      fill=annotation),
      alpha=0.5,
      data=nb.ann[id])+
    geom_rect(aes(
      xmin=min, xmax=max,
      ymin=-Inf, ymax=Inf,
      linetype=status),
      color="black",
      size=2,
      fill="transparent",
      data=selected.err)+
    scale_linetype_manual(
      "error type",
      values=c(
        correct=0,
        "false negative"=3,
        "false positive"=1))+
    scale_fill_manual(
      values=c(breakpoint="violet", normal="orange"))
}
comparePenalties("11.2")
```

---

# Comparing model selection algorithms

```{r}
comparePenalties("11.17")
```

---

# Comparing model selection algorithms

```{r}
comparePenalties("11.4")
```

---

# ROC curves for evaluation

- Point shown is error rate of predicted penalty.
- Add constants to that penalty to trace ROC curve.

```{r, fig.height=7}
auc.dt <- pen.dt[, {
  roc.list <- penaltyLearning::ROChange(nb.err, .SD, "pid.chr")
  with(roc.list, data.table(
    auc,
    thresholds[threshold=="predicted"],
    roc=list(roc)))
}, by=penalty.name]
roc.dt <- auc.dt[, roc[[1]], by=penalty.name]

ggplot()+
  auc.dt[1, ggtitle(sprintf(
    "%d positive + %d negative = %d total labels",
    possible.fn, possible.fp, labels))]+
  geom_path(aes(
    FPR, TPR, color=penalty.name, size=penalty.name),
    data=roc.dt)+
  geom_point(aes(
    FPR, TPR),
    size=3,
    color="grey50",
    data=auc.dt)+
  geom_point(aes(
    FPR, TPR, color=penalty.name),
    data=auc.dt)+
  scale_size_manual(values=c(BIC=1, AIC=2))+
  pen.color.scale+
  theme(legend.position = "none")+
  coord_equal()+
  scale_x_continuous(
    "False Positive Rate (predict change in negative label)")+
  scale_y_continuous(
    "True Positive Rate (predict change in positive label)")+
  directlabels::geom_dl(aes(
    FPR, TPR, color=penalty.name, label=sprintf(
      "%s errors=%d AUC=%.2f", penalty.name, errors, auc)),
    method=list(cex=2, "right.polygons"),
    data=auc.dt)
```

---

# Other ROC curve data visualizations

- http://ml.nau.edu/viz/2021-10-21-curveAlignment/
- http://ml.nau.edu/viz/2021-10-21-neuroblastomaProcessed-complex/

---

# Possible exam questions

- What is the difference between AIC and BIC? 
- Does AIC or BIC tend to select larger model sizes? or do they select
  the same model size? why?
- When using cross-validation for model selection, what set is used as
  input to the learning algorithm? What set is not input to the
  learning algorithm?
- What modifications would be required to use K-fold cross-validation
  instead of a single 50% subtrain, 50% validation split?
- What modifications, if any, would be needed to use these model
  selection algorithms with Hidden Markov Models?


