---
title: "Auto-encoders"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=7)
algo.colors <- c(
  SGD="blue",
  SVD="red")
model.colors <- structure(algo.colors, names=c("Auto-encoder", "PCA"))
suppressPackageStartupMessages({
  if(!requireNamespace("R.utils")){
    install.packages("R.utils")
  }
  library(data.table)
  library(ggplot2)
  ##library(checkpoint)
  ##checkpoint("2020-11-01")
  library(keras)
  ## from https://github.com/rstudio/keras/issues/937
  if(FALSE){
    install.packages("keras")
    keras::install_keras(version = "2.1.6", tensorflow = "1.5")
    ## https://anaconda.org/conda-forge/tensorflow/files?version=1.5.0
    system("conda activate r-reticulate && conda install keras=2.1.6 tensorflow=1.5.0 -c conda-forge")
    system("conda install python=3.6")
    ##/home/tdhock/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/keras/_
    install.packages("../keras_2.3.0.0.tar.gz", repo=NULL)
  }
  keras::use_implementation("keras")
  keras::use_backend("tensorflow")
})
text.size <- 5
theme_set(
  theme_bw()+
    theme(
      panel.spacing=grid::unit(0, "lines"),
      text=element_text(size=20)))
```

# Motivation: MNIST digits data

```{r}
destfile <- "zip.train.gz"
datasets.url <-
  "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/"
if(!file.exists(destfile)){
  zip.url <- paste0(datasets.url, destfile)
  download.file(zip.url, destfile)
}
zip.dt <- data.table::fread(file=destfile)
dim(zip.dt)
zip.label.col.i <- 1
zip.no.label <- as.matrix(
  zip.dt[, -zip.label.col.i, with=FALSE])
n.digits <- 100
obs.i.vec <- 1:n.digits
X.mat <- zip.no.label[obs.i.vec, ]

## 1. MNIST scatterplot of digit labels on first two pca directions (all
## classes).
if(file.exists("figure-fashion-mnist-data.rds")){
  data.list <- readRDS("figure-fashion-mnist-data.rds")
}else{
  data.list <- list(
    fashion=keras::dataset_fashion_mnist(),
    digits=keras::dataset_mnist())
  saveRDS(data.list, "figure-fashion-mnist-data.rds")
}
str(data.list$digits)
X.mat.list <- list()
n.digits <- 100
obs.i.vec <- 1:n.digits
y.vec <- data.list$digits$train$y[obs.i.vec]
X.mat <- matrix(
  data.list$digits$train$x[obs.i.vec, , ],
  n.digits)
image_mat_to_dt <- function(X, n.pixels=28){
  X.t <- t(as.matrix(X))
  data.table(
    digit.i=as.integer(col(X.t)),
    intensity=as.numeric(X.t),
    row=rep(1:n.pixels, n.pixels),
    col=rep(1:n.pixels, each=n.pixels))
}
tile.dt <- image_mat_to_dt(X.mat)
intensity.scale <- scale_fill_gradient2(
  mid="black", high="white", low="red", limits=c(NA, 255))
ggplot()+
  intensity.scale+
  geom_raster(aes(
    col, -row, fill=intensity),
    data=tile.dt)+
  coord_equal()+
  facet_wrap("digit.i", ncol=15)
```

---

# Set of digits is represented as a matrix

- Each digit image in MNIST data set is a matrix of $28\times 28$
  pixel intensity values, $x_i\in\{0,\dots,255\}^{784}$.
- Each of the images is a row in the data matrix.
- Each of the columns is a pixel.
- All images on last slide represented by a data matrix with $n=100$
  rows/images and $p=784$ columns/pixels.

---

# Background/motivation: non-linear dimensionality reduction

- High dimensional data are difficult to visualize.
- For example each observation/example in the MNIST data is of dimension
  28 x 28 = 784 pixels.
- We would like to map each observation into a lower-dimensional
  space for visualization / understanding patterns in the data.
- Principal Components Analysis (PCA) is a linear dimensionality
  reduction method, which is computed using the Singular Value
  Decomposition (SVD).
- Auto-encoders are non-linear, which means they can be more accurate
  than PCA, in terms of reconstruction error.
  
---

# Deep neural networks

- A neural network with $L$ layers is a function $f(x)=f_{L-1}[ ... f_1(x) ]$.
- Each function $f_l(z)=\sigma_l(W_l z)$ consists of multiplication by
  a matrix $W_l$ followed by an activation function $\sigma_l$.
- The number of layers $L$, the sizes of the weight matrices $W_l$,
  and the activation functions $\sigma_l$ are all hyper-parameters
  that must be chosen prior to learning.
- The number of hidden layers is $L-2$ (inputs and outputs are known,
  other layers are not).
- When the number of hidden layers is greater than one, we say that
  the neural network is deep, $L\geq 4$.
- Number of units/features in each layer determines weight matrix
  sizes. To compute $u_{l+1}$ units from $u_{l}$ units the $W_l$
  matrix must be of size $u_{l+1}\times u_l$.
- Sometimes called multi-layer perceptron (MLP).

---

# Auto-encoders are a type of neural network

- The name "auto" is not an abbreviation of automatic; it means that
  the input feature vector $x$ (first layer) is also used as the
  output (last layer).
- Auto-encoders have a middle "code" layer which is the low
  dimensional embedding (typically size 2 for visualization), and
  intermediate layer sizes are typically symmetric.
- The encoder is the first half of functions (from input to code
  layer).
- The decoder is the second half of functions (from code layer to
  output).
- Auto-encoders can have shared weights in the encoder and decoder.
- For example $L=5$ layers for a data set with $p=100$ features the
  number of units per layer could be (100,50,2,50,100).
  
---

# Activation function choices

- linear: $\sigma(z) = z$.
- relu: $\sigma(z) = z\text{ if }z\geq 0\text{ else } 0$.
- sigmoid: $\sigma(z) = 1/(1+e^{-z})$.

```{r fig.height=5}
input <- seq(-3, 3, by=0.1)
fun.list <- list(
  linear=identity,
  relu=function(x)ifelse(x>0, x, 0),
  sigmoid=function(x)1/(1+exp(-x)))
act.dt.list <- list()
for(activation.name in names(fun.list)){
  fun <- fun.list[[activation.name]]
  act.dt.list[[activation.name]] <- data.table(
    activation.name,
    input,
    activation=fun(input))
}
act.dt <- do.call(rbind, act.dt.list)
ggplot()+
  geom_line(aes(
    input, activation),
    data=act.dt)+
  facet_grid(. ~ activation.name, labeller=label_both)
```
  
---

# Auto-encoder learning algorithm

- The goal of learning is to find a low dimensional mapping of the
  data which is able to reconstruct the original data.
- This is measured by the mean squared reconstruction error,
  $\text{MSE}(f) = \frac 1 n \sum_{i=1}^n ||f(x_i) - x_i||_2^2.$
- The values in the weight matrices $W_l$ are the model parameters
  which are learned using the Stochastic Gradient Descent (SGD)
  algorithm.
- Each iteration of SGD updates the weight matrices $W_l$ in order to
  get better predictions (reduce MSE).
- The batch size hyper-parameter is the number of observations for
  which the MSE and its gradient are computed and summed during each
  iteration (step or update to weight matrices).
- An "epoch" involves one or more gradient descent iterations
  (computes gradient with respect to each observation once).
  
---

# Details of Stochastic Gradient Descent (SGD)

- The algorithm starts with
  arbitrary/random weight matrices $W_l$ close to zero.
- The update (one step/iteration) is $W\gets W - \alpha G$ where $W$
  are the weights, $\alpha>0$ is a learning rate/step size
  hyper-parameter, and $G$ is the gradient.
- The gradient is the direction of steepest descent, so the loss/MSE
  is guaranteed to decrease if the step size is small enough.
- But if the step size is too small, then many iterations are required
  to get a small loss/MSE (too slow).
- If step size is too big, then loss/MSE can increase, so you want to
  choose an intermediate step size; best step size depends on the
  problem and data.
  
---
  
# Example: 2d iris data

- Simple example: iris.
- One row for each flower (only 6 of 150 shown below).
- One column for each measurement/dimension.

```{r results=TRUE}
i.df <- iris[, 2:3]
head(i.df)
```

---
  
# Example: 2d iris data

```{r}
gg <- ggplot()+
  geom_point(aes(
    x=Petal.Length, y=Sepal.Width),
    shape=1,
    data=i.df)+
  coord_equal()
gg
```

---

# Auto-encoder neural network architecture

- In the following example the number of units in each layer is (2, 1, 2).
- Input/output layers have two units.
- Code layer has one unit.
- First function has two weights $W_1\in\mathbb R^{1\times 2}$.
- Second function has two weights $W_2\in\mathbb R^{2\times 1}$.
- Linear activation function, so same model as PCA: low-dimensional
  embedding is a linear combination of input features.
- Learning algorithm iteratively searches for best linear model.

---

# Visualization of predicted values

```{r}
n.input.output <- ncol(i.df)
i.model <- keras::keras_model_sequential() %>%
  keras::layer_dense(
    name="code",
    input_shape = n.input.output,
    units = 1, 
    activation = activation_linear) %>%
  keras::layer_dense(
    units = n.input.output)
i.compiled.model <- keras::compile(
  i.model,
  optimizer=keras::optimizer_sgd(lr=0.01),
  loss=keras::loss_mean_squared_error)
i.mat <- as.matrix(i.df)
i.prcomp <- prcomp(i.mat, rank=1)
i.pred.prcomp <- with(i.prcomp, matrix(
  center, nrow(i.mat), ncol(i.mat), byrow=TRUE) + x %*% t(rotation))
i.err.prcomp <- mean((i.pred.prcomp - i.mat)^2)
i.cache.rds <- "13-auto-encoders-iris-results.rds"
if(file.exists(i.cache.rds)){
  iris.results <- readRDS(i.cache.rds)
}else{
  epoch.pred.list <- list()
  epoch.loss.list <- list()
  i.err.nn <- Inf
  epoch <- 0
  while(i.err.nn > i.err.prcomp*1.05){
    epoch <- epoch+1
    fit.history <- keras::fit(
      i.compiled.model, x=i.mat, y=i.mat,
      epochs=1,
      verbose=0)
    i.err.nn <- fit.history[["metrics"]][["loss"]]
    epoch.loss.list[[epoch]] <- data.table(epoch, loss=i.err.nn)
    pred.mat <- predict(i.compiled.model, i.mat)#last layer.
    pred.dt <- data.table(pred.mat)
    setnames(pred.dt, names(i.df))
    epoch.pred.list[[epoch]] <- data.table(epoch, pred.dt)
  }
  epoch.pred <- do.call(rbind, epoch.pred.list)
  epoch.loss <- do.call(rbind, epoch.loss.list)
  iris.results <- list(
    pred=epoch.pred,
    loss=epoch.loss,
    model=i.compiled.model)
  saveRDS(iris.results, i.cache.rds)
}

pca.hline <- data.table(loss=i.err.prcomp)
lambda.intercept <-
  -i.prcomp$center[["Petal.Length"]]/i.prcomp$rotation["Petal.Length",]
abline.dt <- data.table(
  method="slope/intercept",
  intercept=i.prcomp$center[["Sepal.Width"]] +
    lambda.intercept*i.prcomp$rotation["Sepal.Width",],
  slope=i.prcomp$rotation["Sepal.Width",]/i.prcomp$rotation["Petal.Length",])
range.dt <- rbind(
  iris.results$pred[, colnames(i.mat), with=FALSE], i.mat
)[, lapply(.SD, range)]
pca.epoch.plot <- function(show.epoch){
  text.dt <- rbind(
    data.table(algorithm="SGD", iris.results$loss[show.epoch, .(loss)]),
    data.table(algorithm="SVD", loss=i.err.prcomp))
  show.pred <- iris.results$pred[epoch==show.epoch]
  show.segs <- data.table(pred=show.pred, data=i.mat)
  ggplot()+
    ggtitle(paste0("Epoch=", show.epoch))+
    geom_text(aes(
      4, 4,
      color=algorithm,
      label=sprintf(
        "%s loss=%.2f", algorithm, loss),
      vjust=ifelse(algorithm=="SGD", 1.1, -0.1)),
      size=text.size,
      hjust=1,
      data=text.dt)+
    scale_x_continuous(
      "Petal Length",
      limits=range.dt[["Petal.Length"]])+
    scale_y_continuous(
      "Sepal Width",
      limits=range.dt[["Sepal.Width"]])+
    geom_point(aes(
      x=Petal.Length, y=Sepal.Width),
      shape=1,
      data=i.df)+
    coord_equal()+
    scale_color_manual(values=algo.colors)+
    geom_abline(aes(
      slope=slope,
      color=algorithm,
      intercept=intercept),
      data=data.table(abline.dt, algorithm="SVD"))+
    geom_segment(aes(
      data.Petal.Length, data.Sepal.Width,
      xend=pred.Petal.Length, yend=pred.Sepal.Width),
      color="grey",
      alpha=0.5,
      data=data.table(show.segs, algorithm="SGD"))+
    geom_point(aes(
      Petal.Length, Sepal.Width, color=algorithm),
      shape=1,
      data=data.table(show.pred, algorithm="SGD"))
}

pca.epoch.plot(1)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(2)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(3)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(4)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(10)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(100)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(200)
```

---

# Visualization of predicted values

```{r}
pca.epoch.plot(nrow(iris.results$loss))
```

---

# Loss decreases with number of epochs

```{r}
gg <- ggplot()+
  scale_color_manual(values=algo.colors)+
  geom_hline(aes(
    yintercept=loss,
    color=algorithm),
    data=data.table(pca.hline, algorithm="SVD"))+
  scale_y_log10()+
  geom_point(aes(
    epoch, loss, color=algorithm),
    shape=1,
    data=data.table(iris.results$loss, algorithm="SGD"))
gg
```

---

# Zoom to last 100 epochs

```{r}
gg <- ggplot()+
  scale_color_manual(values=algo.colors)+
  geom_hline(aes(
    yintercept=loss,
    color=algorithm),
    data=data.table(pca.hline, algorithm="SVD"))+
  scale_y_log10()+
  geom_point(aes(
    epoch, loss, color=algorithm),
    shape=1,
    data=data.table(iris.results$loss, algorithm="SGD")[seq(.N-100, .N)])
gg
```

---

# Actual image data

```{r}
X.sc <- X.mat/255
X.prcomp <- prcomp(X.sc, rank=2)
X.pred.prcomp <- with(X.prcomp, matrix(
  center, nrow(X.sc), ncol(X.sc), byrow=TRUE) + x %*% t(rotation))
X.err.prcomp <- mean((X.pred.prcomp - X.sc)^2)
n.input.output <- ncol(X.sc)
n.intermediate <- 100
my_activation <- activation_relu
model <- keras::keras_model_sequential() %>%
  keras::layer_dense(
    name="intermediate_1",
    input_shape = n.input.output,
    units = n.intermediate, 
    activation = my_activation) %>%
  keras::layer_dense(
    name="code",
    units = 2,
    activation = my_activation) %>%
  keras::layer_dense(
    name="intermediate_2",
    units = n.intermediate, 
    activation = my_activation) %>%
  keras::layer_dense(
    units = n.input.output,
    activation = my_activation)

m.cache.rds <- "13-auto-encoders-mnist-results.rds"
if(file.exists(m.cache.rds)){
  mnist.results <- readRDS(m.cache.rds)
}else{
  compiled.model <- keras::compile(
    model,
    optimizer=keras::optimizer_sgd(
      lr=1),
    loss=keras::loss_mean_squared_error)
  epoch.pred.list <- list()
  epoch.loss.list <- list()
  X.err.nn <- Inf
  epoch <- 0
  while(X.err.nn > X.err.prcomp*0.85){
    epoch <- epoch+1
    if(X.err.nn < X.err.prcomp*1.1){
      compiled.model <- keras::compile(
        compiled.model,
        optimizer=keras::optimizer_adam(),
        loss=keras::loss_mean_squared_error)
    }
    fit.model <- keras::fit(
      compiled.model, x=X.sc, y=X.sc,
      epochs=1,
      verbose=0)
    X.err.nn <- fit.model[["metrics"]][["loss"]]
    cat(sprintf(
      "epoch=%d NN.err=%f PCA.err=%f\n", epoch, X.err.nn, X.err.prcomp))
    epoch.loss.list[[epoch]] <- data.table(epoch, loss=X.err.nn)
    pred.mat <- predict(compiled.model, X.sc)#last layer.
    pred.dt <- data.table(pred.mat)
    if(epoch %% 100 == 1){
      epoch.pred.list[[epoch]] <- data.table(epoch, pred.dt)
    }
  }
  epoch.pred <- do.call(rbind, epoch.pred.list)
  epoch.loss <- do.call(rbind, epoch.loss.list)
  code_layer_model <- keras::keras_model(
    inputs = compiled.model$input,
    outputs = keras::get_layer(compiled.model, "code")$output)
  code_mat <- predict(code_layer_model, X.mat)
  mnist.results <- list(
    code=code_mat,
    model=compiled.model,
    pred=epoch.pred,
    loss=epoch.loss)
  saveRDS(mnist.results, "13-auto-encoders-mnist-results.rds")
}
intensity.range <- range(mnist.results$pred[,-1], X.pred.prcomp)
grad.n.vec <- scales::rescale(c(
  red=intensity.range[1],
  black=0,
  white=1,
  blue=intensity.range[2]))
reconstructed.images <- function(pred.dt, tit){
  tile.dt <- image_mat_to_dt(pred.dt)
  tile.dt[, label := y.vec[digit.i] ]
  tile.dt[, digit.label := paste0(digit.i, "_", y.vec[digit.i])]
  ggplot()+
    ggtitle(tit)+
    geom_raster(aes(
      col, -row, fill=intensity),
      data=tile.dt)+
    coord_equal()+
    scale_fill_gradientn(
      colours=names(grad.n.vec),
      limits=intensity.range,
      values=grad.n.vec)+
    facet_wrap(~ digit.i + label, ncol=20)
}
images.at.epoch <- function(e){
  reconstructed.images(
    mnist.results$pred[epoch==e,-1],
    mnist.results$loss[epoch==e, sprintf(
    "epoch=%d MSE=%.4f", e, loss)])
}
reconstructed.images(X.sc, "Scaled images")
```

---

# Auto-encoder for image data

- Each image is represented by a vector of 784 pixel intensity values,
  so this is the number of units in the first/last layer.
- The code layer will have 2 units for visualization purposes (two
  axes on a scatterplot).
- There is a choice of the number of intermediate layers; here we
  choose one layer with 100 units (on each side of the code layer).
- Overall model architecture, in terms of number of units/features per
  layer, is (784,100,2,100,784).
- Weight matrix sizes are therefore
  $W_1\in\mathbb R^{100\times 784}, W_2\in\mathbb R^{2\times 100}, 
  W_3\in\mathbb R^{100\times 2}, W_4\in\mathbb R^{784\times 100}$.
- To low-dimensional embedding for an image $x$ is computed via
  $f_2[f_1(x)] = \sigma_2[ W_2 \sigma_1( W_1 x ) ].$

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(1)
```

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(101)
```

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(1001)
```

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(2001)
```

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(3001)
```

---

# Reconstruction improves with epochs of learning

```{r}
images.at.epoch(4001)
```

---

# Reconstruction of PCA

```{r}
reconstructed.images(X.pred.prcomp, paste0("PCA MSE=", X.err.prcomp))
```

---

# Loss versus number of epochs

```{r}
pca.hline <- data.table(loss=X.err.prcomp)
epoch.vlines <- data.table(
  epoch=c(1, 101, 1001, 2001, 3001, 4001))
ggplot()+
  geom_vline(aes(
    xintercept=epoch),
    data=epoch.vlines,
    color="grey")+
  scale_color_manual(values=model.colors)+
  geom_hline(aes(
    yintercept=loss,
    color=model),
    data=data.table(pca.hline, model="PCA"))+
  scale_y_log10()+
  geom_point(aes(
    epoch, loss, color=model),
    shape=1,
    data=data.table(mnist.results$loss, model="Auto-encoder"))
```

---

# Plot code layer variables instead of PCs

```{r}
code.dt <- data.table(code=mnist.results$code, label=factor(y.vec))
ggplot()+
  geom_text(aes(
    code.V1, code.V2, label=label),
    data=code.dt)
```

---

# Possible exam questions

- What choices do you need to make in the auto-encoder in order to
  have the result be the same as PCA?
- What is the total number of parameters for an auto-encoder of a data
  set with $p=100$ features, if we use 2 code units and 10
  intermediate units? (assume only weight matrices, no bias/intercept
  to learn)
